{"cells":[{"cell_type":"markdown","metadata":{"id":"q4Y3d3ZKD3qI"},"source":["# Gesture Recognition\n","In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started.\n","\n","By \n","Richard Kanagaraj \n","MD MirMohsin"]},{"cell_type":"code","source":["pip install scipy==1.1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEryRulOJOpZ","outputId":"eeadb1b0-9e31-43a1-a814-00b90efb584d","executionInfo":{"status":"ok","timestamp":1676235042514,"user_tz":300,"elapsed":808542,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.1.0\n","  Downloading scipy-1.1.0.tar.gz (15.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: scipy\n","  Building wheel for scipy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scipy: filename=scipy-1.1.0-cp38-cp38-linux_x86_64.whl size=54550568 sha256=04e35702275f157d66e7cc63be4fd419d078a9c95b464978c25acf45f5ac524c\n","  Stored in directory: /root/.cache/pip/wheels/29/a9/5e/9e4eddde37a1e15cf5cb404ba197df482cc39ffbfef91ec337\n","Successfully built scipy\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.7.3\n","    Uninstalling scipy-1.7.3:\n","      Successfully uninstalled scipy-1.7.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray-einstats 0.5.1 requires scipy>=1.6, but you have scipy 1.1.0 which is incompatible.\n","pymc 4.1.4 requires scipy>=1.4.1, but you have scipy 1.1.0 which is incompatible.\n","plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.1.0 which is incompatible.\n","jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.1.0 which is incompatible.\n","jax 0.3.25 requires scipy>=1.5, but you have scipy 1.1.0 which is incompatible.\n","aeppl 0.0.33 requires scipy>=1.4.0, but you have scipy 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scipy-1.1.0\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"NJsmqEVqD3qM","executionInfo":{"status":"ok","timestamp":1676236485020,"user_tz":300,"elapsed":1686,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"outputs":[],"source":["import numpy as np\n","import os \n","from imageio import imread\n","#from scipy.misc import imread, imresize\n","from skimage.transform import resize\n","import datetime\n","import os\n","import cv2"]},{"cell_type":"markdown","metadata":{"id":"2tZbO6HhD3qO"},"source":["We set the random seed so that the results don't vary drastically."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RnTbMnaJD3qO","executionInfo":{"status":"ok","timestamp":1676236497127,"user_tz":300,"elapsed":2363,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"outputs":[],"source":["np.random.seed(30)\n","import random as rn\n","rn.seed(30)\n","from keras import backend as K\n","import tensorflow as tf\n","tf.random.set_seed(30)"]},{"cell_type":"markdown","metadata":{"id":"hNS5zgMrD3qP"},"source":["In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PE1xk3W-LJ3","outputId":"4c4ecaba-037e-482f-ab39-e3352f17efe0","executionInfo":{"status":"ok","timestamp":1676236518812,"user_tz":300,"elapsed":19764,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"d4VvmTKiD3qP","executionInfo":{"status":"ok","timestamp":1676236523576,"user_tz":300,"elapsed":1296,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"outputs":[],"source":["train_doc = np.random.permutation(open('/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train.csv').readlines())\n","val_doc = np.random.permutation(open('/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/val.csv').readlines())\n","batch_size = 20"]},{"cell_type":"markdown","metadata":{"id":"EQ0RfnezD3qQ"},"source":["## Generator\n","This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"JVRC3XlkD3qQ","executionInfo":{"status":"ok","timestamp":1676236531309,"user_tz":300,"elapsed":161,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"outputs":[],"source":["def generator(source_path, folder_list, batch_size):\n","    print( 'Source path = ', source_path, '; batch size =', batch_size)\n","    img_idx = np.round(np.linspace(0,29,16)).astype(int) #create a list of image numbers you want to use for a particular video\n","    while True:\n","        t = np.random.permutation(folder_list)\n","        num_batches = len(folder_list)//batch_size # calculate the number of batches\n","        for batch in range(num_batches): # we iterate over the number of batches\n","            batch_data = np.zeros((batch_size,len(img_idx),120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n","            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n","            for folder in range(batch_size): # iterate over the batch_size\n","                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n","                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n","                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","                    \n","                    #crop the images and resize them. Note that the images are of 2 different shape \n","                    #and the conv3D will throw error if the inputs in a batch have different shapes\n","                    \n","                    image = resize(image,(120,120))\n","\n","                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n","                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n","                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n","                    \n","                    \n","                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n","            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n","        rem_image = len(folder_list)%batch_size\n","        batch += 1\n","        if(rem_image!=0):\n","            batch_data = np.zeros((rem_image,len(img_idx),120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n","            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n","            for folder in range(rem_image): # iterate over the batch_size\n","                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n","                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n","                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","                    \n","                    #crop the images and resize them. Note that the images are of 2 different shape \n","                    #and the conv3D will throw error if the inputs in a batch have different shapes\n","                   \n","                    image = resize(image,(120,120))\n","                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n","                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n","                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n","                    \n","                    \n","                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n","            yield batch_data, batch_labels"]},{"cell_type":"markdown","metadata":{"id":"LuWSPVH3D3qR"},"source":["Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"GmqvfnnpD3qR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7344aef6-8cd3-4e18-b999-97f8429ca1ac","executionInfo":{"status":"ok","timestamp":1676236535559,"user_tz":300,"elapsed":192,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["# training sequences = 663\n","# validation sequences = 100\n","# epochs = 20\n"]}],"source":["curr_dt_time = datetime.datetime.now()\n","train_path = '/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train'\n","val_path = '/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/val'\n","num_train_sequences = len(train_doc)\n","print('# training sequences =', num_train_sequences)\n","num_val_sequences = len(val_doc)\n","print('# validation sequences =', num_val_sequences)\n","num_epochs = 20 # choose the number of epochs\n","print ('# epochs =', num_epochs)\n","num_classes = 5"]},{"cell_type":"markdown","metadata":{"id":"8Z8GkqKPD3qS"},"source":["## Model\n","Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."]},{"cell_type":"markdown","source":["## Model 1 - CONV3D Model"],"metadata":{"id":"XFlRcBfW_wkh"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"x9Z8ePhLD3qS","executionInfo":{"status":"ok","timestamp":1676236551011,"user_tz":300,"elapsed":4386,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"outputs":[],"source":["import keras as Keras\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n","from keras.layers.convolutional import Conv3D, MaxPooling3D, AveragePooling3D\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras import optimizers\n","from keras.regularizers import l2\n","from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n","from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n","from keras.optimizers import Adam\n","\n","filtersize=(3,3,3)\n","dropout=0.5\n","dense_neurons=256\n","\n","\n","model = Sequential()\n","\n","model.add(Conv3D(16, filtersize, padding='same',input_shape=(16,120,120,3)))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(16, filtersize, padding='same',input_shape=(16,120,120,3)))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","model.add(Conv3D(32, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(32, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","model.add(Conv3D(64, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(64, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","model.add(Conv3D(128, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(128, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        \n","\n","model.add(Flatten())\n","model.add(Dense(dense_neurons,activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(dropout))\n","\n","model.add(Dense(dense_neurons,activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(dropout))\n","\n","model.add(Dense(num_classes,activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"yaOg6J-sD3qT"},"source":["Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."]},{"cell_type":"code","source":["optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002)#write your optimizer\n","model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","print (model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzO4LN_o_6Zy","outputId":"6ff93f1d-05f0-4206-b59f-07b17106df1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv3d (Conv3D)             (None, 16, 120, 120, 16)  1312      \n","                                                                 \n"," activation (Activation)     (None, 16, 120, 120, 16)  0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 16, 120, 120, 16)  64       \n"," ormalization)                                                   \n","                                                                 \n"," conv3d_1 (Conv3D)           (None, 16, 120, 120, 16)  6928      \n","                                                                 \n"," activation_1 (Activation)   (None, 16, 120, 120, 16)  0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 16, 120, 120, 16)  64       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d (MaxPooling3D  (None, 8, 60, 60, 16)    0         \n"," )                                                               \n","                                                                 \n"," conv3d_2 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n","                                                                 \n"," activation_2 (Activation)   (None, 8, 60, 60, 32)     0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 8, 60, 60, 32)    128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv3d_3 (Conv3D)           (None, 8, 60, 60, 32)     27680     \n","                                                                 \n"," activation_3 (Activation)   (None, 8, 60, 60, 32)     0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 8, 60, 60, 32)    128       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d_1 (MaxPooling  (None, 4, 30, 30, 32)    0         \n"," 3D)                                                             \n","                                                                 \n"," conv3d_4 (Conv3D)           (None, 4, 30, 30, 64)     55360     \n","                                                                 \n"," activation_4 (Activation)   (None, 4, 30, 30, 64)     0         \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 4, 30, 30, 64)    256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv3d_5 (Conv3D)           (None, 4, 30, 30, 64)     110656    \n","                                                                 \n"," activation_5 (Activation)   (None, 4, 30, 30, 64)     0         \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 4, 30, 30, 64)    256       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d_2 (MaxPooling  (None, 2, 15, 15, 64)    0         \n"," 3D)                                                             \n","                                                                 \n"," conv3d_6 (Conv3D)           (None, 2, 15, 15, 128)    221312    \n","                                                                 \n"," activation_6 (Activation)   (None, 2, 15, 15, 128)    0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 2, 15, 15, 128)   512       \n"," hNormalization)                                                 \n","                                                                 \n"," conv3d_7 (Conv3D)           (None, 2, 15, 15, 128)    442496    \n","                                                                 \n"," activation_7 (Activation)   (None, 2, 15, 15, 128)    0         \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 2, 15, 15, 128)   512       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d_3 (MaxPooling  (None, 1, 7, 7, 128)     0         \n"," 3D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dense (Dense)               (None, 256)               1605888   \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               65792     \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 5)                 1285      \n","                                                                 \n","=================================================================\n","Total params: 2,556,533\n","Trainable params: 2,554,549\n","Non-trainable params: 1,984\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","metadata":{"id":"WkE75xRLD3qT"},"source":["Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEuZ1usLD3qT"},"outputs":[],"source":["train_generator = generator(train_path, train_doc, batch_size)\n","val_generator = generator(val_path, val_doc, batch_size)"]},{"cell_type":"code","source":[],"metadata":{"id":"8o6BwBRPcEu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5-YLlpID3qT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be19e35a-2798-43f4-befa-6a13c0e11e62"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"]}],"source":["model_name = 'final_conv3d_model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n","    \n","if not os.path.exists(model_name):\n","    os.mkdir(model_name)\n","        \n","filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","\n","LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n","\n","callbacks_list = [checkpoint, LR]"]},{"cell_type":"markdown","metadata":{"id":"nTu_PkgWD3qU"},"source":["The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVQSLSO0D3qU"},"outputs":[],"source":["if (num_train_sequences%batch_size) == 0:\n","    steps_per_epoch = int(num_train_sequences/batch_size)\n","else:\n","    steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","if (num_val_sequences%batch_size) == 0:\n","    validation_steps = int(num_val_sequences/batch_size)\n","else:\n","    validation_steps = (num_val_sequences//batch_size) + 1"]},{"cell_type":"markdown","metadata":{"id":"B8zyLIrcD3qU"},"source":["Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9WplGMJD3qU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"020f2fb2-49b9-4b57-dc46-1f0900fc4e6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Source path =  /content/drive/MyDrive/new/train ; batch size = 20\n","Epoch 1/20\n","34/34 [==============================] - ETA: 0s - loss: 2.0756 - categorical_accuracy: 0.3544  Source path =  /content/drive/MyDrive/new/val ; batch size = 20\n","\n","Epoch 1: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00001-2.07558-0.35445-1.95015-0.16000.h5\n","34/34 [==============================] - 4993s 151s/step - loss: 2.0756 - categorical_accuracy: 0.3544 - val_loss: 1.9501 - val_categorical_accuracy: 0.1600 - lr: 2.0000e-04\n","Epoch 2/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5734 - categorical_accuracy: 0.4615\n","Epoch 2: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00002-1.57341-0.46154-3.16348-0.13000.h5\n","34/34 [==============================] - 176s 5s/step - loss: 1.5734 - categorical_accuracy: 0.4615 - val_loss: 3.1635 - val_categorical_accuracy: 0.1300 - lr: 2.0000e-04\n","Epoch 3/20\n","34/34 [==============================] - ETA: 0s - loss: 1.4303 - categorical_accuracy: 0.5385\n","Epoch 3: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00003-1.43034-0.53846-3.82784-0.16000.h5\n","\n","Epoch 3: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n","34/34 [==============================] - 176s 5s/step - loss: 1.4303 - categorical_accuracy: 0.5385 - val_loss: 3.8278 - val_categorical_accuracy: 0.1600 - lr: 2.0000e-04\n","Epoch 4/20\n","34/34 [==============================] - ETA: 0s - loss: 1.1570 - categorical_accuracy: 0.6094\n","Epoch 4: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00004-1.15699-0.60935-4.64876-0.17000.h5\n","34/34 [==============================] - 165s 5s/step - loss: 1.1570 - categorical_accuracy: 0.6094 - val_loss: 4.6488 - val_categorical_accuracy: 0.1700 - lr: 1.0000e-04\n","Epoch 5/20\n","34/34 [==============================] - ETA: 0s - loss: 1.0489 - categorical_accuracy: 0.6471\n","Epoch 5: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00005-1.04891-0.64706-5.35255-0.19000.h5\n","\n","Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n","34/34 [==============================] - 167s 5s/step - loss: 1.0489 - categorical_accuracy: 0.6471 - val_loss: 5.3525 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-04\n","Epoch 6/20\n","34/34 [==============================] - ETA: 0s - loss: 0.9243 - categorical_accuracy: 0.6606\n","Epoch 6: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00006-0.92426-0.66063-5.50326-0.16000.h5\n","34/34 [==============================] - 175s 5s/step - loss: 0.9243 - categorical_accuracy: 0.6606 - val_loss: 5.5033 - val_categorical_accuracy: 0.1600 - lr: 5.0000e-05\n","Epoch 7/20\n","34/34 [==============================] - ETA: 0s - loss: 0.8614 - categorical_accuracy: 0.6772\n","Epoch 7: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00007-0.86138-0.67722-5.30896-0.16000.h5\n","\n","Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n","34/34 [==============================] - 178s 5s/step - loss: 0.8614 - categorical_accuracy: 0.6772 - val_loss: 5.3090 - val_categorical_accuracy: 0.1600 - lr: 5.0000e-05\n","Epoch 8/20\n","34/34 [==============================] - ETA: 0s - loss: 0.8222 - categorical_accuracy: 0.7014\n","Epoch 8: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00008-0.82215-0.70136-5.67426-0.25000.h5\n","34/34 [==============================] - 175s 5s/step - loss: 0.8222 - categorical_accuracy: 0.7014 - val_loss: 5.6743 - val_categorical_accuracy: 0.2500 - lr: 2.5000e-05\n","Epoch 9/20\n","34/34 [==============================] - ETA: 0s - loss: 0.8187 - categorical_accuracy: 0.6983\n","Epoch 9: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00009-0.81867-0.69834-4.50746-0.33000.h5\n","\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n","34/34 [==============================] - 176s 5s/step - loss: 0.8187 - categorical_accuracy: 0.6983 - val_loss: 4.5075 - val_categorical_accuracy: 0.3300 - lr: 2.5000e-05\n","Epoch 10/20\n","34/34 [==============================] - ETA: 0s - loss: 0.7998 - categorical_accuracy: 0.7059\n","Epoch 10: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00010-0.79982-0.70588-5.69903-0.24000.h5\n","34/34 [==============================] - 165s 5s/step - loss: 0.7998 - categorical_accuracy: 0.7059 - val_loss: 5.6990 - val_categorical_accuracy: 0.2400 - lr: 1.2500e-05\n","Epoch 11/20\n","34/34 [==============================] - ETA: 0s - loss: 0.7497 - categorical_accuracy: 0.7315\n","Epoch 11: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00011-0.74973-0.73152-4.80594-0.30000.h5\n","\n","Epoch 11: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n","34/34 [==============================] - 164s 5s/step - loss: 0.7497 - categorical_accuracy: 0.7315 - val_loss: 4.8059 - val_categorical_accuracy: 0.3000 - lr: 1.2500e-05\n","Epoch 12/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6670 - categorical_accuracy: 0.7360\n","Epoch 12: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00012-0.66704-0.73605-4.40305-0.32000.h5\n","34/34 [==============================] - 165s 5s/step - loss: 0.6670 - categorical_accuracy: 0.7360 - val_loss: 4.4030 - val_categorical_accuracy: 0.3200 - lr: 6.2500e-06\n","Epoch 13/20\n","34/34 [==============================] - ETA: 0s - loss: 0.7419 - categorical_accuracy: 0.7436\n","Epoch 13: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00013-0.74186-0.74359-3.56672-0.29000.h5\n","\n","Epoch 13: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n","34/34 [==============================] - 176s 5s/step - loss: 0.7419 - categorical_accuracy: 0.7436 - val_loss: 3.5667 - val_categorical_accuracy: 0.2900 - lr: 6.2500e-06\n","Epoch 14/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6517 - categorical_accuracy: 0.7647\n","Epoch 14: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00014-0.65167-0.76471-2.78461-0.32000.h5\n","34/34 [==============================] - 176s 5s/step - loss: 0.6517 - categorical_accuracy: 0.7647 - val_loss: 2.7846 - val_categorical_accuracy: 0.3200 - lr: 3.1250e-06\n","Epoch 15/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6570 - categorical_accuracy: 0.7496\n","Epoch 15: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00015-0.65696-0.74962-2.53797-0.33000.h5\n","\n","Epoch 15: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n","34/34 [==============================] - 176s 5s/step - loss: 0.6570 - categorical_accuracy: 0.7496 - val_loss: 2.5380 - val_categorical_accuracy: 0.3300 - lr: 3.1250e-06\n","Epoch 16/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6279 - categorical_accuracy: 0.7768\n","Epoch 16: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00016-0.62795-0.77677-1.88358-0.42000.h5\n","34/34 [==============================] - 175s 5s/step - loss: 0.6279 - categorical_accuracy: 0.7768 - val_loss: 1.8836 - val_categorical_accuracy: 0.4200 - lr: 1.5625e-06\n","Epoch 17/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6795 - categorical_accuracy: 0.7496\n","Epoch 17: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00017-0.67947-0.74962-1.47808-0.48000.h5\n","34/34 [==============================] - 164s 5s/step - loss: 0.6795 - categorical_accuracy: 0.7496 - val_loss: 1.4781 - val_categorical_accuracy: 0.4800 - lr: 1.5625e-06\n","Epoch 18/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6353 - categorical_accuracy: 0.7587\n","Epoch 18: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00018-0.63529-0.75867-1.20508-0.58000.h5\n","34/34 [==============================] - 178s 5s/step - loss: 0.6353 - categorical_accuracy: 0.7587 - val_loss: 1.2051 - val_categorical_accuracy: 0.5800 - lr: 1.5625e-06\n","Epoch 19/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6071 - categorical_accuracy: 0.7979\n","Epoch 19: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00019-0.60713-0.79789-1.04785-0.58000.h5\n","34/34 [==============================] - 177s 5s/step - loss: 0.6071 - categorical_accuracy: 0.7979 - val_loss: 1.0479 - val_categorical_accuracy: 0.5800 - lr: 1.5625e-06\n","Epoch 20/20\n","34/34 [==============================] - ETA: 0s - loss: 0.7014 - categorical_accuracy: 0.7421\n","Epoch 20: saving model to final_conv3d_model_2023-02-1215_45_24.411617/model-00020-0.70141-0.74208-0.83893-0.69000.h5\n","34/34 [==============================] - 168s 5s/step - loss: 0.7014 - categorical_accuracy: 0.7421 - val_loss: 0.8389 - val_categorical_accuracy: 0.6900 - lr: 1.5625e-06\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f99fc16cdc0>"]},"metadata":{},"execution_count":13}],"source":["model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n","                    callbacks=callbacks_list, validation_data=val_generator, \n","                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"]},{"cell_type":"markdown","source":["## Model 2 - CONV2D + LSTM Model"],"metadata":{"id":"jr0xFgy8d1Xm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dn7L-xZhD3qV"},"outputs":[],"source":["model_2 = Sequential()\n","\n","model_2.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=(16, 120, 120, 3)))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","model_2.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","model_2.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","model_2.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","model_2.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","model_2.add(TimeDistributed(Flatten()))\n","model_2.add(LSTM(256))\n","model_2.add(Dropout(0.25))\n","        \n","model_2.add(Dense(128,activation='relu'))\n","model_2.add(Dropout(0.25))\n","model_2.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"code","source":["optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002) #write your optimizer\n","model_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","print (model_2.summary())"],"metadata":{"id":"5Qe8-aX-eEKw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0896bfcc-6200-443e-9473-4ac1989710b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," time_distributed (TimeDistr  (None, 16, 120, 120, 16)  448      \n"," ibuted)                                                         \n","                                                                 \n"," time_distributed_1 (TimeDis  (None, 16, 120, 120, 16)  64       \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_2 (TimeDis  (None, 16, 60, 60, 16)   0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_3 (TimeDis  (None, 16, 60, 60, 32)   4640      \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_4 (TimeDis  (None, 16, 60, 60, 32)   128       \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_5 (TimeDis  (None, 16, 30, 30, 32)   0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_6 (TimeDis  (None, 16, 30, 30, 64)   18496     \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_7 (TimeDis  (None, 16, 30, 30, 64)   256       \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_8 (TimeDis  (None, 16, 15, 15, 64)   0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_9 (TimeDis  (None, 16, 15, 15, 128)  73856     \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_10 (TimeDi  (None, 16, 15, 15, 128)  512       \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_11 (TimeDi  (None, 16, 7, 7, 128)    0         \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_12 (TimeDi  (None, 16, 7, 7, 256)    295168    \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_13 (TimeDi  (None, 16, 7, 7, 256)    1024      \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_14 (TimeDi  (None, 16, 3, 3, 256)    0         \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_15 (TimeDi  (None, 16, 2304)         0         \n"," stributed)                                                      \n","                                                                 \n"," lstm (LSTM)                 (None, 256)               2622464   \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 128)               32896     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 5)                 645       \n","                                                                 \n","=================================================================\n","Total params: 3,050,597\n","Trainable params: 3,049,605\n","Non-trainable params: 992\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","source":["Let us create the **train_generator** and the **val_generator** which will be used in .fitand not fit_generator as it is deprecated and will be removed in a future version."],"metadata":{"id":"SDD-zQTueIuV"}},{"cell_type":"code","source":["train_generator = generator(train_path, train_doc, batch_size)\n","val_generator = generator(val_path, val_doc, batch_size)"],"metadata":{"id":"SOvhpSlMeN7N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'final_conv2d+lstm_model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n","    \n","if not os.path.exists(model_name):\n","    os.mkdir(model_name)\n","        \n","filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n","\n","LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n","        \n","callbacks_list = [checkpoint, LR]"],"metadata":{"id":"wBaE3S-FeQcd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e90f2a79-78f5-40ac-8b29-c9dbb6a9ae97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"]}]},{"cell_type":"code","source":["if (num_train_sequences%batch_size) == 0:\n","    steps_per_epoch = int(num_train_sequences/batch_size)\n","else:\n","    steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","if (num_val_sequences%batch_size) == 0:\n","    validation_steps = int(num_val_sequences/batch_size)\n","else:\n","    validation_steps = (num_val_sequences//batch_size) + 1"],"metadata":{"id":"dqyDy5QEeSel"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us now fit the model. This will start training the model and with the help of the checkpoints, we'll be able to save the model at the end of each epoch."],"metadata":{"id":"MliihesjeUjt"}},{"cell_type":"code","source":["model_2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n","                    callbacks=callbacks_list, validation_data=val_generator, \n","                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"],"metadata":{"id":"v6PndJxveV-M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"619fe964-34fc-4d0b-fa20-2b1a7009d80d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Source path =  /content/drive/MyDrive/new/train ; batch size = 20\n","Epoch 1/20\n","34/34 [==============================] - ETA: 0s - loss: 1.4571 - categorical_accuracy: 0.3801Source path =  /content/drive/MyDrive/new/val ; batch size = 20\n","\n","Epoch 1: val_loss improved from inf to 1.61091, saving model to final_conv2d+lstm_model_2023-02-1215_45_24.411617/model-00001-1.45707-0.38009-1.61091-0.23000.h5\n","34/34 [==============================] - 170s 5s/step - loss: 1.4571 - categorical_accuracy: 0.3801 - val_loss: 1.6109 - val_categorical_accuracy: 0.2300 - lr: 2.0000e-04\n","Epoch 2/20\n","34/34 [==============================] - ETA: 0s - loss: 1.0247 - categorical_accuracy: 0.6109\n","Epoch 2: val_loss did not improve from 1.61091\n","34/34 [==============================] - 161s 5s/step - loss: 1.0247 - categorical_accuracy: 0.6109 - val_loss: 1.6406 - val_categorical_accuracy: 0.1900 - lr: 2.0000e-04\n","Epoch 3/20\n","34/34 [==============================] - ETA: 0s - loss: 0.7793 - categorical_accuracy: 0.7149\n","Epoch 3: val_loss improved from 1.61091 to 1.60186, saving model to final_conv2d+lstm_model_2023-02-1215_45_24.411617/model-00003-0.77926-0.71493-1.60186-0.36000.h5\n","34/34 [==============================] - 173s 5s/step - loss: 0.7793 - categorical_accuracy: 0.7149 - val_loss: 1.6019 - val_categorical_accuracy: 0.3600 - lr: 2.0000e-04\n","Epoch 4/20\n","34/34 [==============================] - ETA: 0s - loss: 0.6483 - categorical_accuracy: 0.7496\n","Epoch 4: val_loss did not improve from 1.60186\n","34/34 [==============================] - 163s 5s/step - loss: 0.6483 - categorical_accuracy: 0.7496 - val_loss: 1.7253 - val_categorical_accuracy: 0.1700 - lr: 2.0000e-04\n","Epoch 5/20\n","34/34 [==============================] - ETA: 0s - loss: 0.5064 - categorical_accuracy: 0.8160\n","Epoch 5: val_loss did not improve from 1.60186\n","\n","Epoch 5: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n","34/34 [==============================] - 160s 5s/step - loss: 0.5064 - categorical_accuracy: 0.8160 - val_loss: 1.6972 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n","Epoch 6/20\n","34/34 [==============================] - ETA: 0s - loss: 0.3827 - categorical_accuracy: 0.8884\n","Epoch 6: val_loss did not improve from 1.60186\n","34/34 [==============================] - 174s 5s/step - loss: 0.3827 - categorical_accuracy: 0.8884 - val_loss: 1.7287 - val_categorical_accuracy: 0.2100 - lr: 1.0000e-04\n","Epoch 7/20\n","34/34 [==============================] - ETA: 0s - loss: 0.2751 - categorical_accuracy: 0.9367\n","Epoch 7: val_loss did not improve from 1.60186\n","\n","Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n","34/34 [==============================] - 164s 5s/step - loss: 0.2751 - categorical_accuracy: 0.9367 - val_loss: 1.7361 - val_categorical_accuracy: 0.2800 - lr: 1.0000e-04\n","Epoch 8/20\n","34/34 [==============================] - ETA: 0s - loss: 0.2075 - categorical_accuracy: 0.9608\n","Epoch 8: val_loss did not improve from 1.60186\n","34/34 [==============================] - 174s 5s/step - loss: 0.2075 - categorical_accuracy: 0.9608 - val_loss: 1.9371 - val_categorical_accuracy: 0.1900 - lr: 5.0000e-05\n","Epoch 9/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1763 - categorical_accuracy: 0.9608\n","Epoch 9: val_loss did not improve from 1.60186\n","\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n","34/34 [==============================] - 175s 5s/step - loss: 0.1763 - categorical_accuracy: 0.9608 - val_loss: 1.8662 - val_categorical_accuracy: 0.2500 - lr: 5.0000e-05\n","Epoch 10/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1376 - categorical_accuracy: 0.9864\n","Epoch 10: val_loss did not improve from 1.60186\n","34/34 [==============================] - 172s 5s/step - loss: 0.1376 - categorical_accuracy: 0.9864 - val_loss: 2.1351 - val_categorical_accuracy: 0.2400 - lr: 2.5000e-05\n","Epoch 11/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1347 - categorical_accuracy: 0.9759\n","Epoch 11: val_loss did not improve from 1.60186\n","\n","Epoch 11: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n","34/34 [==============================] - 161s 5s/step - loss: 0.1347 - categorical_accuracy: 0.9759 - val_loss: 2.1487 - val_categorical_accuracy: 0.3100 - lr: 2.5000e-05\n","Epoch 12/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1187 - categorical_accuracy: 0.9849\n","Epoch 12: val_loss did not improve from 1.60186\n","34/34 [==============================] - 162s 5s/step - loss: 0.1187 - categorical_accuracy: 0.9849 - val_loss: 2.1822 - val_categorical_accuracy: 0.3100 - lr: 1.2500e-05\n","Epoch 13/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1198 - categorical_accuracy: 0.9819\n","Epoch 13: val_loss did not improve from 1.60186\n","\n","Epoch 13: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n","34/34 [==============================] - 173s 5s/step - loss: 0.1198 - categorical_accuracy: 0.9819 - val_loss: 2.0093 - val_categorical_accuracy: 0.3500 - lr: 1.2500e-05\n","Epoch 14/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1105 - categorical_accuracy: 0.9879\n","Epoch 14: val_loss did not improve from 1.60186\n","34/34 [==============================] - 173s 5s/step - loss: 0.1105 - categorical_accuracy: 0.9879 - val_loss: 1.8931 - val_categorical_accuracy: 0.3800 - lr: 6.2500e-06\n","Epoch 15/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1137 - categorical_accuracy: 0.9864\n","Epoch 15: val_loss did not improve from 1.60186\n","\n","Epoch 15: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n","34/34 [==============================] - 162s 5s/step - loss: 0.1137 - categorical_accuracy: 0.9864 - val_loss: 1.9773 - val_categorical_accuracy: 0.4100 - lr: 6.2500e-06\n","Epoch 16/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1064 - categorical_accuracy: 0.9894\n","Epoch 16: val_loss did not improve from 1.60186\n","34/34 [==============================] - 173s 5s/step - loss: 0.1064 - categorical_accuracy: 0.9894 - val_loss: 1.6625 - val_categorical_accuracy: 0.4800 - lr: 3.1250e-06\n","Epoch 17/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0999 - categorical_accuracy: 0.9925\n","Epoch 17: val_loss improved from 1.60186 to 1.30900, saving model to final_conv2d+lstm_model_2023-02-1215_45_24.411617/model-00017-0.09995-0.99246-1.30900-0.59000.h5\n","34/34 [==============================] - 171s 5s/step - loss: 0.0999 - categorical_accuracy: 0.9925 - val_loss: 1.3090 - val_categorical_accuracy: 0.5900 - lr: 3.1250e-06\n","Epoch 18/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0961 - categorical_accuracy: 0.9910\n","Epoch 18: val_loss did not improve from 1.30900\n","34/34 [==============================] - 162s 5s/step - loss: 0.0961 - categorical_accuracy: 0.9910 - val_loss: 1.3113 - val_categorical_accuracy: 0.5200 - lr: 3.1250e-06\n","Epoch 19/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1012 - categorical_accuracy: 0.9894\n","Epoch 19: val_loss improved from 1.30900 to 1.03037, saving model to final_conv2d+lstm_model_2023-02-1215_45_24.411617/model-00019-0.10119-0.98944-1.03037-0.63000.h5\n","34/34 [==============================] - 172s 5s/step - loss: 0.1012 - categorical_accuracy: 0.9894 - val_loss: 1.0304 - val_categorical_accuracy: 0.6300 - lr: 3.1250e-06\n","Epoch 20/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1006 - categorical_accuracy: 0.9894\n","Epoch 20: val_loss did not improve from 1.03037\n","34/34 [==============================] - 172s 5s/step - loss: 0.1006 - categorical_accuracy: 0.9894 - val_loss: 1.0598 - val_categorical_accuracy: 0.6100 - lr: 3.1250e-06\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f99f005b730>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## 4 / 5 and 6 Model Class added with newer approach\n"],"metadata":{"id":"-NkKw2lYTjaX"}},{"cell_type":"markdown","source":["### Generator Class\n","This is one of the most important part of the code. The overall structure of the generator is broken down into modules. In the generator, we are going to preprocess the images as we have images of 2 different dimensions as well as create a batch of video frames."],"metadata":{"id":"Rfgj6sS7Tod6"}},{"cell_type":"code","source":["class DataGenerator:\n","    def __init__(self, width=120, height=120, frames=30, channel=3, \n","                 crop = True, normalize = False, affine = False, flip = False, edge = False  ):\n","        self.width = width   # X dimension of the image\n","        self.height = height # Y dimesnion of the image\n","        self.frames = frames # length/depth of the video frames\n","        self.channel = channel # number of channels in images 3 for color(RGB) and 1 for Gray  \n","        self.affine = affine # augment data with affine transform of the image\n","        self.flip = flip\n","        self.normalize =  normalize\n","        self.edge = edge # edge detection\n","        self.crop = crop\n","\n","    # Helper function to generate a random affine transform on the image\n","    def __get_random_affine(self): # private method\n","        dx, dy = np.random.randint(-1.7, 1.8, 2)\n","        M = np.float32([[1, 0, dx], [0, 1, dy]])\n","        return M\n","\n","    # Helper function to initialize all the batch image data and labels\n","    def __init_batch_data(self, batch_size): # private method\n","        batch_data = np.zeros((batch_size, self.frames, self.width, self.height, self.channel)) \n","        batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n","        return batch_data, batch_labels\n","\n","    def __load_batch_images(self, source_path, folder_list, batch_num, batch_size, t): # private method\n","    \n","        batch_data,batch_labels = self.__init_batch_data(batch_size)\n","        # We will also build a agumented batch data\n","        if self.affine:\n","            batch_data_aug,batch_labels_aug = self.__init_batch_data(batch_size)\n","        if self.flip:\n","            batch_data_flip,batch_labels_flip = self.__init_batch_data(batch_size)\n","\n","        #create a list of image numbers you want to use for a particular video\n","        img_idx = [x for x in range(0, self.frames)] \n","\n","        for folder in range(batch_size): # iterate over the batch_size\n","            # read all the images in the folder\n","            imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch_num*batch_size)].split(';')[0])) \n","            # Generate a random affine to be used in image transformation for buidling agumented data set\n","            M = self.__get_random_affine()\n","            \n","            #  Iterate over the frames/images of a folder to read them in\n","            for idx, item in enumerate(img_idx): \n","                image = cv2.imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n","                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","                \n","                #crop the images and resize them. Note that the images are of 2 different shape \n","                #and the conv3D will throw error if the inputs in a batch have different shapes  \n","                if self.crop:\n","                    image = self.__crop(image)\n","                # If normalize is set normalize the image else use the raw image.\n","                if self.normalize:\n","                    resized = self.__normalize(self.__resize(image))\n","                else:\n","                    resized = self.__resize(image)\n","                # If the input is edge detected image then use the sobelx, sobely and laplacian as 3 channel of the edge detected image\n","                if self.edge:\n","                    resized = self.__edge(resized)\n","                \n","                batch_data[folder,idx] = resized\n","                if self.affine:\n","                    batch_data_aug[folder,idx] = self.__affine(resized, M)   \n","                if self.flip:\n","                    batch_data_flip[folder,idx] = self.__flip(resized)   \n","\n","            batch_labels[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n","            \n","            if self.affine:\n","                batch_labels_aug[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n","            \n","            if self.flip:\n","                if int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==0:\n","                    batch_labels_flip[folder, 1] = 1\n","                elif int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==1:\n","                    batch_labels_flip[folder, 0] = 1\n","                else:\n","                    batch_labels_flip[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n","        \n","        if self.affine:\n","            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n","            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n","        if self.flip:\n","            batch_data = np.append(batch_data, batch_data_flip, axis = 0) \n","            batch_labels = np.append(batch_labels, batch_labels_flip, axis = 0) \n","\n","        return batch_data, batch_labels\n","    \n","    def generator(self, source_path, folder_list, batch_size): # public method\n","        print( 'Source path = ', source_path, '; batch size =', batch_size)\n","        while True:\n","            t = np.random.permutation(folder_list)\n","            num_batches = len(folder_list)//batch_size # calculate the number of batches\n","            for batch in range(num_batches): # we iterate over the number of batches\n","                # you yield the batch_data and the batch_labels, remember what does yield do\n","                yield self.__load_batch_images(source_path, folder_list, batch, batch_size, t) \n","            \n","            # Code for the remaining data points which are left after full batches\n","            if (len(folder_list) != batch_size*num_batches):\n","                batch_size = len(folder_list) - (batch_size*num_batches)\n","                yield self.__load_batch_images(source_path, folder_list, num_batches, batch_size, t)\n","\n","    # Helper function to perfom affice transform on the image\n","    def __affine(self, image, M):\n","        return cv2.warpAffine(image, M, (image.shape[0], image.shape[1]))\n","\n","    # Helper function to flip the image\n","    def __flip(self, image):\n","        return np.flip(image,1)\n","    \n","    # Helper function to normalise the data\n","    def __normalize(self, image):\n","        return image/127.5-1\n","    \n","    # Helper function to resize the image\n","    def __resize(self, image):\n","        return cv2.resize(image, (self.width,self.height), interpolation = cv2.INTER_AREA)\n","    \n","    # Helper function to crop the image\n","    def __crop(self, image):\n","        if image.shape[0] != image.shape[1]:\n","            return image[0:120, 20:140]\n","        else:\n","            return image\n","\n","    # Helper function for edge detection\n","    def __edge(self, image):\n","        edge = np.zeros((image.shape[0], image.shape[1], image.shape[2]))\n","        edge[:,:,0] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,0],(3,3),0),cv2.CV_64F)\n","        edge[:,:,1] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,1],(3,3),0),cv2.CV_64F)\n","        edge[:,:,2] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,2],(3,3),0),cv2.CV_64F)\n","        return edge"],"metadata":{"id":"gKw00U76TuHn","executionInfo":{"status":"ok","timestamp":1676236563243,"user_tz":300,"elapsed":150,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Model Class as common component"],"metadata":{"id":"PTpdcuWKUR0h"}},{"cell_type":"code","source":["class ModelGenerator(object):\n","        \n","    @classmethod\n","    def c3d3(cls, input_shape, nb_classes):\n","        model = Sequential()\n","        model.add(Conv3D(16, kernel_size=(3, 3, 3), input_shape=input_shape, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(Conv3D(16, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Flatten())\n","        model.add(Dense(512, activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model\n"," \n","    @classmethod\n","    def c3d4(cls, input_shape, nb_classes):\n","        # Define model\n","        model = Sequential()\n","\n","        model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        #Flatten Layers\n","        model.add(Flatten())\n","\n","        model.add(Dense(256, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        #softmax layer\n","        model.add(Dense(5, activation='softmax'))\n","        return model\n","    \n","    @classmethod\n","    def c3d5(cls, input_shape, nb_classes):\n","        # Define model\n","        model = Sequential()\n","\n","        model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))\n","        #model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n","        #model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n","        #model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        #Flatten Layers\n","        model.add(Flatten())\n","\n","        model.add(Dense(256, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        #softmax layer\n","        model.add(Dense(5, activation='softmax'))\n","        return model   \n","    \n","    @classmethod\n","    def lstm(cls, input_shape, nb_classes):\n","        \"\"\"Build a simple LSTM network. We pass the extracted features from\n","        our CNN to this model predomenently.\"\"\"\n","        # Model.\n","        model = Sequential()\n","        model.add(LSTM(2048, return_sequences=False,\n","                       input_shape=input_shape,\n","                       dropout=0.5))\n","        model.add(Dense(512, activation='relu'))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model\n","\n","    @classmethod\n","    def lrcn(cls, input_shape, nb_classes):\n","        \"\"\"Build a CNN into RNN.\n","        Starting version from:\n","            https://github.com/udacity/self-driving-car/blob/master/\n","                steering-models/community-models/chauffeur/models.py\n","        Heavily influenced by VGG-16:\n","            https://arxiv.org/abs/1409.1556\n","        Also known as an LRCN:\n","            https://arxiv.org/pdf/1411.4389.pdf\n","        \"\"\"\n","        model = Sequential()\n","\n","        model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2),\n","            activation='relu', padding='same'), input_shape=input_shape))\n","        model.add(TimeDistributed(Conv2D(32, (3,3),\n","            kernel_initializer=\"he_normal\", activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Conv2D(64, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(64, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Conv2D(128, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(128, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Conv2D(256, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(256, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(512, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(512, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Flatten()))\n","\n","        model.add(Dropout(0.5))\n","        model.add(LSTM(256, return_sequences=False, dropout=0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model\n","\n","    @classmethod\n","    def mlp(cls, input_shape, nb_classes):\n","        \"\"\"Build a simple MLP. It uses extracted features as the input\n","        because of the otherwise too-high dimensionality.\"\"\"\n","        # Model.\n","        model = Sequential()\n","        model.add(Flatten(input_shape=input_shape))\n","        model.add(Dense(512))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(512))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model"],"metadata":{"id":"D2NFDe_LTui_","executionInfo":{"status":"ok","timestamp":1676236566804,"user_tz":300,"elapsed":173,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Common Trainer Function"],"metadata":{"id":"pZEPW05OUpkg"}},{"cell_type":"code","source":["def train(batch_size, num_epochs, model, train_generator, val_generator, optimiser=None):\n","\n","    curr_dt_time = datetime.datetime.now()\n","\n","    num_train_sequences = len(train_doc)\n","    print('# training sequences =', num_train_sequences)\n","    num_val_sequences = len(val_doc)\n","    print('# validation sequences =', num_val_sequences)\n","    print('# batch size =', batch_size)    \n","    print('# epochs =', num_epochs)\n","\n","    #optimizer = Adam(lr=rate) \n","    #write your optimizer\n","    if optimiser == None:\n","        optimiser = Adam() \n","    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","    print (model.summary())\n","    \n","    model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n","    \n","    if not os.path.exists(model_name):\n","        os.mkdir(model_name)\n","            \n","    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","    checkpoint = ModelCheckpoint(filepath, \n","                                 monitor='val_loss', \n","                                 verbose=1, \n","                                 save_best_only=False, \n","                                 save_weights_only=False, \n","                                 mode='auto', \n","                                 period=1)\n","    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n","    callbacks_list = [checkpoint, LR]\n","\n","    if (num_train_sequences%batch_size) == 0:\n","        steps_per_epoch = int(num_train_sequences/batch_size)\n","    else:\n","        steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","    if (num_val_sequences%batch_size) == 0:\n","        validation_steps = int(num_val_sequences/batch_size)\n","    else:\n","        validation_steps = (num_val_sequences//batch_size) + 1\n","\n","    model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n","                callbacks=callbacks_list, validation_data=val_generator, \n","                validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n","    \n","    K.clear_session()"],"metadata":{"id":"VA-DhkuRUX0i","executionInfo":{"status":"ok","timestamp":1676236570164,"user_tz":300,"elapsed":165,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from numba import cuda\n","def clear_cuda():\n","    #cuda.select_device(0)\n","    #cuda.close()\n","    pass"],"metadata":{"id":"123PV8T0UtJD","executionInfo":{"status":"ok","timestamp":1676236573888,"user_tz":300,"elapsed":936,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Model 3"],"metadata":{"id":"VbaqniOPav0G"}},{"cell_type":"code","source":["train_gen = DataGenerator()\n","val_gen = DataGenerator()\n","model_gen = ModelGenerator()\n","\n","input_shape = (30,120,120, 3)\n","num_classes = 5\n","\n","model = model_gen.c3d3(input_shape, num_classes)\n","\n","#batch_size = 20\n","#num_epochs = 20\n","\n","train_generator = train_gen.generator(train_path, train_doc, batch_size)\n","val_generator = val_gen.generator(val_path, val_doc, batch_size)\n","train(batch_size, num_epochs, model, train_generator, val_generator)"],"metadata":{"id":"ynWcFHHcUvA1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676241934272,"user_tz":300,"elapsed":754447,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"}},"outputId":"f914a447-342b-4c48-a38f-a799a9a80f9e"},"execution_count":12,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["# training sequences = 663\n","# validation sequences = 100\n","# batch size = 20\n","# epochs = 20\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv3d_8 (Conv3D)           (None, 30, 120, 120, 16)  1312      \n","                                                                 \n"," activation_8 (Activation)   (None, 30, 120, 120, 16)  0         \n","                                                                 \n"," conv3d_9 (Conv3D)           (None, 30, 120, 120, 16)  6928      \n","                                                                 \n"," activation_9 (Activation)   (None, 30, 120, 120, 16)  0         \n","                                                                 \n"," max_pooling3d_4 (MaxPooling  (None, 10, 40, 40, 16)   0         \n"," 3D)                                                             \n","                                                                 \n"," dropout_2 (Dropout)         (None, 10, 40, 40, 16)    0         \n","                                                                 \n"," conv3d_10 (Conv3D)          (None, 10, 40, 40, 32)    13856     \n","                                                                 \n"," activation_10 (Activation)  (None, 10, 40, 40, 32)    0         \n","                                                                 \n"," conv3d_11 (Conv3D)          (None, 10, 40, 40, 32)    27680     \n","                                                                 \n"," activation_11 (Activation)  (None, 10, 40, 40, 32)    0         \n","                                                                 \n"," max_pooling3d_5 (MaxPooling  (None, 4, 14, 14, 32)    0         \n"," 3D)                                                             \n","                                                                 \n"," dropout_3 (Dropout)         (None, 4, 14, 14, 32)     0         \n","                                                                 \n"," conv3d_12 (Conv3D)          (None, 4, 14, 14, 32)     27680     \n","                                                                 \n"," activation_12 (Activation)  (None, 4, 14, 14, 32)     0         \n","                                                                 \n"," conv3d_13 (Conv3D)          (None, 4, 14, 14, 32)     27680     \n","                                                                 \n"," activation_13 (Activation)  (None, 4, 14, 14, 32)     0         \n","                                                                 \n"," max_pooling3d_6 (MaxPooling  (None, 2, 5, 5, 32)      0         \n"," 3D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 2, 5, 5, 32)       0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 1600)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 512)               819712    \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 512)              2048      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_5 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 929,461\n","Trainable params: 928,437\n","Non-trainable params: 1,024\n","_________________________________________________________________\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["None\n","Source path =  /content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train ; batch size = 20\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-10-555dec366f02>:46: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","34/34 [==============================] - ETA: 0s - loss: 2.1266 - categorical_accuracy: 0.2745  Source path =  /content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/val ; batch size = 20\n","\n","Epoch 1: saving model to model_init_2023-02-1221_16_18.395556/model-00001-2.12656-0.27451-22.35892-0.23000.h5\n","34/34 [==============================] - 4723s 143s/step - loss: 2.1266 - categorical_accuracy: 0.2745 - val_loss: 22.3589 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n","Epoch 2/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6817 - categorical_accuracy: 0.3431\n","Epoch 2: saving model to model_init_2023-02-1221_16_18.395556/model-00002-1.68168-0.34314-11.03671-0.13000.h5\n","34/34 [==============================] - 26s 790ms/step - loss: 1.6817 - categorical_accuracy: 0.3431 - val_loss: 11.0367 - val_categorical_accuracy: 0.1300 - lr: 0.0010\n","Epoch 3/20\n","34/34 [==============================] - ETA: 0s - loss: 2.0598 - categorical_accuracy: 0.3333\n","Epoch 3: saving model to model_init_2023-02-1221_16_18.395556/model-00003-2.05983-0.33333-4.01946-0.25000.h5\n","34/34 [==============================] - 25s 762ms/step - loss: 2.0598 - categorical_accuracy: 0.3333 - val_loss: 4.0195 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n","Epoch 4/20\n","34/34 [==============================] - ETA: 0s - loss: 2.0127 - categorical_accuracy: 0.3235\n","Epoch 4: saving model to model_init_2023-02-1221_16_18.395556/model-00004-2.01271-0.32353-4.13661-0.25000.h5\n","34/34 [==============================] - 32s 971ms/step - loss: 2.0127 - categorical_accuracy: 0.3235 - val_loss: 4.1366 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n","Epoch 5/20\n","34/34 [==============================] - ETA: 0s - loss: 1.9745 - categorical_accuracy: 0.2941\n","Epoch 5: saving model to model_init_2023-02-1221_16_18.395556/model-00005-1.97448-0.29412-19.50519-0.21000.h5\n","\n","Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","34/34 [==============================] - 32s 973ms/step - loss: 1.9745 - categorical_accuracy: 0.2941 - val_loss: 19.5052 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n","Epoch 6/20\n","34/34 [==============================] - 26s 779ms/step - loss: 2.0164 - categorical_accuracy: 0.2451 - val_loss: 6.6112 - val_categorical_accuracy: 0.2300 - lr: 5.0000e-04\n","Epoch 7/20\n","34/34 [==============================] - ETA: 0s - loss: 1.8060 - categorical_accuracy: 0.2549\n","Epoch 7: saving model to model_init_2023-02-1221_16_18.395556/model-00007-1.80598-0.25490-2.56391-0.24000.h5\n","34/34 [==============================] - 32s 959ms/step - loss: 1.8060 - categorical_accuracy: 0.2549 - val_loss: 2.5639 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-04\n","Epoch 8/20\n","34/34 [==============================] - ETA: 0s - loss: 1.9534 - categorical_accuracy: 0.2549\n","Epoch 8: saving model to model_init_2023-02-1221_16_18.395556/model-00008-1.95337-0.25490-1.61594-0.28000.h5\n","34/34 [==============================] - 26s 798ms/step - loss: 1.9534 - categorical_accuracy: 0.2549 - val_loss: 1.6159 - val_categorical_accuracy: 0.2800 - lr: 5.0000e-04\n","Epoch 9/20\n","34/34 [==============================] - ETA: 0s - loss: 1.9302 - categorical_accuracy: 0.2059\n","Epoch 9: saving model to model_init_2023-02-1221_16_18.395556/model-00009-1.93020-0.20588-1.70385-0.32000.h5\n","34/34 [==============================] - 26s 777ms/step - loss: 1.9302 - categorical_accuracy: 0.2059 - val_loss: 1.7039 - val_categorical_accuracy: 0.3200 - lr: 5.0000e-04\n","Epoch 10/20\n","34/34 [==============================] - ETA: 0s - loss: 1.8337 - categorical_accuracy: 0.2745\n","Epoch 10: saving model to model_init_2023-02-1221_16_18.395556/model-00010-1.83366-0.27451-1.83945-0.23000.h5\n","\n","Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","34/34 [==============================] - 26s 789ms/step - loss: 1.8337 - categorical_accuracy: 0.2745 - val_loss: 1.8395 - val_categorical_accuracy: 0.2300 - lr: 5.0000e-04\n","Epoch 11/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5640 - categorical_accuracy: 0.3725\n","Epoch 11: saving model to model_init_2023-02-1221_16_18.395556/model-00011-1.56399-0.37255-1.96412-0.29000.h5\n","34/34 [==============================] - 26s 792ms/step - loss: 1.5640 - categorical_accuracy: 0.3725 - val_loss: 1.9641 - val_categorical_accuracy: 0.2900 - lr: 2.5000e-04\n","Epoch 12/20\n","34/34 [==============================] - ETA: 0s - loss: 1.8944 - categorical_accuracy: 0.3137\n","Epoch 12: saving model to model_init_2023-02-1221_16_18.395556/model-00012-1.89438-0.31373-1.88654-0.27000.h5\n","\n","Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","34/34 [==============================] - 26s 789ms/step - loss: 1.8944 - categorical_accuracy: 0.3137 - val_loss: 1.8865 - val_categorical_accuracy: 0.2700 - lr: 2.5000e-04\n","Epoch 13/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5885 - categorical_accuracy: 0.3431\n","Epoch 13: saving model to model_init_2023-02-1221_16_18.395556/model-00013-1.58849-0.34314-1.99197-0.28000.h5\n","34/34 [==============================] - 26s 788ms/step - loss: 1.5885 - categorical_accuracy: 0.3431 - val_loss: 1.9920 - val_categorical_accuracy: 0.2800 - lr: 1.2500e-04\n","Epoch 14/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6883 - categorical_accuracy: 0.3627\n","Epoch 14: saving model to model_init_2023-02-1221_16_18.395556/model-00014-1.68831-0.36275-2.04529-0.30000.h5\n","\n","Epoch 14: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","34/34 [==============================] - 26s 773ms/step - loss: 1.6883 - categorical_accuracy: 0.3627 - val_loss: 2.0453 - val_categorical_accuracy: 0.3000 - lr: 1.2500e-04\n","Epoch 15/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6225 - categorical_accuracy: 0.3235\n","Epoch 15: saving model to model_init_2023-02-1221_16_18.395556/model-00015-1.62248-0.32353-1.98927-0.28000.h5\n","34/34 [==============================] - 26s 786ms/step - loss: 1.6225 - categorical_accuracy: 0.3235 - val_loss: 1.9893 - val_categorical_accuracy: 0.2800 - lr: 6.2500e-05\n","Epoch 16/20\n","34/34 [==============================] - ETA: 0s - loss: 1.7227 - categorical_accuracy: 0.3235\n","Epoch 16: saving model to model_init_2023-02-1221_16_18.395556/model-00016-1.72269-0.32353-2.26296-0.28000.h5\n","\n","Epoch 16: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","34/34 [==============================] - 25s 761ms/step - loss: 1.7227 - categorical_accuracy: 0.3235 - val_loss: 2.2630 - val_categorical_accuracy: 0.2800 - lr: 6.2500e-05\n","Epoch 17/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5731 - categorical_accuracy: 0.4412\n","Epoch 17: saving model to model_init_2023-02-1221_16_18.395556/model-00017-1.57313-0.44118-2.13206-0.24000.h5\n","34/34 [==============================] - 26s 777ms/step - loss: 1.5731 - categorical_accuracy: 0.4412 - val_loss: 2.1321 - val_categorical_accuracy: 0.2400 - lr: 3.1250e-05\n","Epoch 18/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6747 - categorical_accuracy: 0.3529\n","Epoch 18: saving model to model_init_2023-02-1221_16_18.395556/model-00018-1.67471-0.35294-1.73629-0.31000.h5\n","\n","Epoch 18: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","34/34 [==============================] - 25s 769ms/step - loss: 1.6747 - categorical_accuracy: 0.3529 - val_loss: 1.7363 - val_categorical_accuracy: 0.3100 - lr: 3.1250e-05\n","Epoch 19/20\n","34/34 [==============================] - ETA: 0s - loss: 1.8324 - categorical_accuracy: 0.2941\n","Epoch 19: saving model to model_init_2023-02-1221_16_18.395556/model-00019-1.83244-0.29412-1.52792-0.36000.h5\n","34/34 [==============================] - 26s 794ms/step - loss: 1.8324 - categorical_accuracy: 0.2941 - val_loss: 1.5279 - val_categorical_accuracy: 0.3600 - lr: 1.5625e-05\n","Epoch 20/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6366 - categorical_accuracy: 0.2647\n","Epoch 20: saving model to model_init_2023-02-1221_16_18.395556/model-00020-1.63664-0.26471-1.44365-0.41000.h5\n","34/34 [==============================] - 26s 789ms/step - loss: 1.6366 - categorical_accuracy: 0.2647 - val_loss: 1.4436 - val_categorical_accuracy: 0.4100 - lr: 1.5625e-05\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Modx-pLrayBH"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[],"machine_shape":"hm"},"gpuClass":"premium","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}