{"cells":[{"cell_type":"markdown","metadata":{"id":"q4Y3d3ZKD3qI"},"source":["# Gesture Recognition\n","In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started.\n","\n","By \n","Richard Kanagaraj \n","MD MirMohsin"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1276,"status":"ok","timestamp":1676225718459,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"cEryRulOJOpZ","outputId":"b265ca99-d6e9-4195-89e7-c1c274995def"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement scipy--1.1.0 (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for scipy--1.1.0\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["pip install scipy--1.1.0"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1087,"status":"ok","timestamp":1676225715418,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"NJsmqEVqD3qM"},"outputs":[],"source":["import numpy as np\n","import os \n","from imageio import imread\n","#from scipy.misc import imread, imresize\n","from skimage.transform import resize\n","import datetime\n","import os"]},{"cell_type":"markdown","metadata":{"id":"2tZbO6HhD3qO"},"source":["We set the random seed so that the results don't vary drastically."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2358,"status":"ok","timestamp":1676225722787,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"RnTbMnaJD3qO"},"outputs":[],"source":["np.random.seed(30)\n","import random as rn\n","rn.seed(30)\n","from keras import backend as K\n","import tensorflow as tf\n","tf.random.set_seed(30)"]},{"cell_type":"markdown","metadata":{"id":"hNS5zgMrD3qP"},"source":["In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15778,"status":"ok","timestamp":1676225740217,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"2PE1xk3W-LJ3","outputId":"2d03eb93-afba-4eef-cf28-52c60c851d0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1839,"status":"ok","timestamp":1676225743513,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"d4VvmTKiD3qP"},"outputs":[],"source":["train_doc = np.random.permutation(open('/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train.csv').readlines())\n","val_doc = np.random.permutation(open('/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/val.csv').readlines())\n","batch_size = 20"]},{"cell_type":"markdown","metadata":{"id":"EQ0RfnezD3qQ"},"source":["## Generator\n","This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":114,"status":"ok","timestamp":1676225775943,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"JVRC3XlkD3qQ"},"outputs":[],"source":["def generator(source_path, folder_list, batch_size):\n","    print( 'Source path = ', source_path, '; batch size =', batch_size)\n","    img_idx = np.round(np.linspace(0,29,16)).astype(int) #create a list of image numbers you want to use for a particular video\n","    while True:\n","        t = np.random.permutation(folder_list)\n","        num_batches = len(folder_list)//batch_size # calculate the number of batches\n","        for batch in range(num_batches): # we iterate over the number of batches\n","            batch_data = np.zeros((batch_size,len(img_idx),120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n","            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n","            for folder in range(batch_size): # iterate over the batch_size\n","                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n","                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n","                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","                    \n","                    #crop the images and resize them. Note that the images are of 2 different shape \n","                    #and the conv3D will throw error if the inputs in a batch have different shapes\n","                    \n","                    image = resize(image,(120,120))\n","\n","                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n","                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n","                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n","                    \n","                    \n","                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n","            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n","        rem_image = len(folder_list)%batch_size\n","        batch += 1\n","        if(rem_image!=0):\n","            batch_data = np.zeros((rem_image,len(img_idx),120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n","            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n","            for folder in range(rem_image): # iterate over the batch_size\n","                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n","                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n","                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n","                    \n","                    #crop the images and resize them. Note that the images are of 2 different shape \n","                    #and the conv3D will throw error if the inputs in a batch have different shapes\n","                   \n","                    image = resize(image,(120,120))\n","                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n","                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n","                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n","                    \n","                    \n","                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n","            yield batch_data, batch_labels"]},{"cell_type":"markdown","metadata":{"id":"LuWSPVH3D3qR"},"source":["Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1676225779592,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"GmqvfnnpD3qR","outputId":"d59d4a63-a7e0-42e6-c288-898ca4a202f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["# training sequences = 663\n","# validation sequences = 100\n","# epochs = 20\n"]}],"source":["curr_dt_time = datetime.datetime.now()\n","train_path = '/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train'\n","val_path = '/content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/val'\n","num_train_sequences = len(train_doc)\n","print('# training sequences =', num_train_sequences)\n","num_val_sequences = len(val_doc)\n","print('# validation sequences =', num_val_sequences)\n","num_epochs = 20 # choose the number of epochs\n","print ('# epochs =', num_epochs)\n","num_classes = 5"]},{"cell_type":"markdown","metadata":{"id":"8Z8GkqKPD3qS"},"source":["## Model\n","Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."]},{"cell_type":"markdown","metadata":{"id":"XFlRcBfW_wkh"},"source":["## Model 1 - CONV3D Model"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":423,"status":"ok","timestamp":1676228206634,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"x9Z8ePhLD3qS"},"outputs":[],"source":["import keras as Keras\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n","from keras.layers.convolutional import Conv3D, MaxPooling3D, AveragePooling3D\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras import optimizers\n","from keras.regularizers import l2\n","from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n","from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n","from keras.optimizers import Adam\n","\n","\n","filtersize=(3,3,3)\n","dropout=0.5\n","dense_neurons=256\n","\n","\n","model = Sequential()\n","\n","model.add(Conv3D(16, filtersize, padding='same',input_shape=(16,120,120,3)))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(16, filtersize, padding='same',input_shape=(16,120,120,3)))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","model.add(Conv3D(32, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(32, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","model.add(Conv3D(64, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(64, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","model.add(Conv3D(128, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(Conv3D(128, filtersize, padding='same'))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","        \n","model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","        \n","\n","model.add(Flatten())\n","model.add(Dense(dense_neurons,activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(dropout))\n","\n","model.add(Dense(dense_neurons,activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(dropout))\n","\n","model.add(Dense(num_classes,activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"yaOg6J-sD3qT"},"source":["Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":451,"status":"ok","timestamp":1676225791305,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"kzO4LN_o_6Zy","outputId":"b530735d-6f9c-4587-f5f9-7230a2cbc30c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv3d (Conv3D)             (None, 16, 120, 120, 16)  1312      \n","                                                                 \n"," activation (Activation)     (None, 16, 120, 120, 16)  0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 16, 120, 120, 16)  64       \n"," ormalization)                                                   \n","                                                                 \n"," conv3d_1 (Conv3D)           (None, 16, 120, 120, 16)  6928      \n","                                                                 \n"," activation_1 (Activation)   (None, 16, 120, 120, 16)  0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 16, 120, 120, 16)  64       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d (MaxPooling3D  (None, 8, 60, 60, 16)    0         \n"," )                                                               \n","                                                                 \n"," conv3d_2 (Conv3D)           (None, 8, 60, 60, 32)     13856     \n","                                                                 \n"," activation_2 (Activation)   (None, 8, 60, 60, 32)     0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 8, 60, 60, 32)    128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv3d_3 (Conv3D)           (None, 8, 60, 60, 32)     27680     \n","                                                                 \n"," activation_3 (Activation)   (None, 8, 60, 60, 32)     0         \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 8, 60, 60, 32)    128       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d_1 (MaxPooling  (None, 4, 30, 30, 32)    0         \n"," 3D)                                                             \n","                                                                 \n"," conv3d_4 (Conv3D)           (None, 4, 30, 30, 64)     55360     \n","                                                                 \n"," activation_4 (Activation)   (None, 4, 30, 30, 64)     0         \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 4, 30, 30, 64)    256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv3d_5 (Conv3D)           (None, 4, 30, 30, 64)     110656    \n","                                                                 \n"," activation_5 (Activation)   (None, 4, 30, 30, 64)     0         \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 4, 30, 30, 64)    256       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d_2 (MaxPooling  (None, 2, 15, 15, 64)    0         \n"," 3D)                                                             \n","                                                                 \n"," conv3d_6 (Conv3D)           (None, 2, 15, 15, 128)    221312    \n","                                                                 \n"," activation_6 (Activation)   (None, 2, 15, 15, 128)    0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 2, 15, 15, 128)   512       \n"," hNormalization)                                                 \n","                                                                 \n"," conv3d_7 (Conv3D)           (None, 2, 15, 15, 128)    442496    \n","                                                                 \n"," activation_7 (Activation)   (None, 2, 15, 15, 128)    0         \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 2, 15, 15, 128)   512       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling3d_3 (MaxPooling  (None, 1, 7, 7, 128)     0         \n"," 3D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dense (Dense)               (None, 256)               1605888   \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               65792     \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 5)                 1285      \n","                                                                 \n","=================================================================\n","Total params: 2,556,533\n","Trainable params: 2,554,549\n","Non-trainable params: 1,984\n","_________________________________________________________________\n","None\n"]}],"source":["optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002)#write your optimizer\n","model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","print (model.summary())"]},{"cell_type":"markdown","metadata":{"id":"WkE75xRLD3qT"},"source":["Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":123,"status":"ok","timestamp":1676225796218,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"eEuZ1usLD3qT"},"outputs":[],"source":["train_generator = generator(train_path, train_doc, batch_size)\n","val_generator = generator(val_path, val_doc, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8o6BwBRPcEu6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1676225798974,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"t5-YLlpID3qT","outputId":"a4792b65-7d32-4f2e-e426-be1fa12b5fe4"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"]}],"source":["model_name = 'final_conv3d_model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n","    \n","if not os.path.exists(model_name):\n","    os.mkdir(model_name)\n","        \n","filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n","\n","LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n","\n","callbacks_list = [checkpoint, LR]"]},{"cell_type":"markdown","metadata":{"id":"nTu_PkgWD3qU"},"source":["The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1676225801575,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"gVQSLSO0D3qU"},"outputs":[],"source":["if (num_train_sequences%batch_size) == 0:\n","    steps_per_epoch = int(num_train_sequences/batch_size)\n","else:\n","    steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","if (num_val_sequences%batch_size) == 0:\n","    validation_steps = int(num_val_sequences/batch_size)\n","else:\n","    validation_steps = (num_val_sequences//batch_size) + 1"]},{"cell_type":"markdown","metadata":{"id":"B8zyLIrcD3qU"},"source":["Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBH9AAU2ztnf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":1368618,"status":"error","timestamp":1676227172835,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"H9WplGMJD3qU","outputId":"7e4cf8c7-0729-407e-acbf-aabf8492e90b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Source path =  /content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train ; batch size = 20\n","Epoch 1/20\n","18/34 [==============\u003e...............] - ETA: 18:58 - loss: 2.2460 - categorical_accuracy: 0.3056"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-14-f60efb873de8\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--\u003e 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n","                    callbacks=callbacks_list, validation_data=val_generator, \n","                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"]},{"cell_type":"markdown","metadata":{"id":"U6-3TUYEJBQu"},"source":[]},{"cell_type":"markdown","metadata":{"id":"jr0xFgy8d1Xm"},"source":["## Model 2 - CONV2D + LSTM Model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1450,"status":"ok","timestamp":1676213806567,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"dn7L-xZhD3qV"},"outputs":[],"source":["model_2 = Sequential()\n","\n","model_2.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=(16, 120, 120, 3)))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","model_2.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","model_2.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","model_2.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","        \n","model_2.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n","model_2.add(TimeDistributed(BatchNormalization()))\n","model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n","model_2.add(TimeDistributed(Flatten()))\n","model_2.add(LSTM(256))\n","model_2.add(Dropout(0.25))\n","        \n","model_2.add(Dense(128,activation='relu'))\n","model_2.add(Dropout(0.25))\n","model_2.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":477,"status":"ok","timestamp":1676213811455,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"5Qe8-aX-eEKw","outputId":"73d7481c-badd-4074-ccb6-28d5570ae644"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," time_distributed (TimeDistr  (None, 16, 120, 120, 16)  448      \n"," ibuted)                                                         \n","                                                                 \n"," time_distributed_1 (TimeDis  (None, 16, 120, 120, 16)  64       \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_2 (TimeDis  (None, 16, 60, 60, 16)   0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_3 (TimeDis  (None, 16, 60, 60, 32)   4640      \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_4 (TimeDis  (None, 16, 60, 60, 32)   128       \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_5 (TimeDis  (None, 16, 30, 30, 32)   0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_6 (TimeDis  (None, 16, 30, 30, 64)   18496     \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_7 (TimeDis  (None, 16, 30, 30, 64)   256       \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_8 (TimeDis  (None, 16, 15, 15, 64)   0         \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_9 (TimeDis  (None, 16, 15, 15, 128)  73856     \n"," tributed)                                                       \n","                                                                 \n"," time_distributed_10 (TimeDi  (None, 16, 15, 15, 128)  512       \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_11 (TimeDi  (None, 16, 7, 7, 128)    0         \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_12 (TimeDi  (None, 16, 7, 7, 256)    295168    \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_13 (TimeDi  (None, 16, 7, 7, 256)    1024      \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_14 (TimeDi  (None, 16, 3, 3, 256)    0         \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_15 (TimeDi  (None, 16, 2304)         0         \n"," stributed)                                                      \n","                                                                 \n"," lstm (LSTM)                 (None, 256)               2622464   \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 128)               32896     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 5)                 645       \n","                                                                 \n","=================================================================\n","Total params: 3,050,597\n","Trainable params: 3,049,605\n","Non-trainable params: 992\n","_________________________________________________________________\n","None\n"]}],"source":["optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002) #write your optimizer\n","model_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","print (model_2.summary())"]},{"cell_type":"markdown","metadata":{"id":"SDD-zQTueIuV"},"source":["Let us create the **train_generator** and the **val_generator** which will be used in .fitand not fit_generator as it is deprecated and will be removed in a future version."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":286,"status":"ok","timestamp":1676213878207,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"SOvhpSlMeN7N"},"outputs":[],"source":["train_generator = generator(train_path, train_doc, batch_size)\n","val_generator = generator(val_path, val_doc, batch_size)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":473,"status":"ok","timestamp":1676213881089,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"wBaE3S-FeQcd","outputId":"247ed5dd-5b9f-4981-f432-aa77a8398f16"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"]}],"source":["model_name = 'final_conv2d+lstm_model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n","    \n","if not os.path.exists(model_name):\n","    os.mkdir(model_name)\n","        \n","filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n","\n","LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n","        \n","callbacks_list = [checkpoint, LR]"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1676213891222,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"dqyDy5QEeSel"},"outputs":[],"source":["if (num_train_sequences%batch_size) == 0:\n","    steps_per_epoch = int(num_train_sequences/batch_size)\n","else:\n","    steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","if (num_val_sequences%batch_size) == 0:\n","    validation_steps = int(num_val_sequences/batch_size)\n","else:\n","    validation_steps = (num_val_sequences//batch_size) + 1"]},{"cell_type":"markdown","metadata":{"id":"MliihesjeUjt"},"source":["Let us now fit the model. This will start training the model and with the help of the checkpoints, we'll be able to save the model at the end of each epoch."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676213956708,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"z2PnL9p1IqkM"},"outputs":[],"source":["# setting the match to 2 and checking and see if the system can perform \n","batch_size = 2"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11133303,"status":"ok","timestamp":1676225094708,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"v6PndJxveV-M","outputId":"cb22c410-1b9d-4ee1-e251-9220d98dbe18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Source path =  /content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train ; batch size = 20\n","Epoch 1/20\n","34/34 [==============================] - ETA: 0s - loss: 1.4162 - categorical_accuracy: 0.3831 Source path =  /content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/val ; batch size = 20\n","\n","Epoch 1: val_loss improved from inf to 1.62177, saving model to final_conv2d+lstm_model_2023-02-1204_52_58.729297/model-00001-1.41618-0.38311-1.62177-0.21000.h5\n","34/34 [==============================] - 563s 16s/step - loss: 1.4162 - categorical_accuracy: 0.3831 - val_loss: 1.6218 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n","Epoch 2/20\n","34/34 [==============================] - ETA: 0s - loss: 0.9829 - categorical_accuracy: 0.6561 \n","Epoch 2: val_loss did not improve from 1.62177\n","34/34 [==============================] - 585s 17s/step - loss: 0.9829 - categorical_accuracy: 0.6561 - val_loss: 1.6332 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n","Epoch 3/20\n","34/34 [==============================] - ETA: 0s - loss: 0.7306 - categorical_accuracy: 0.7451 \n","Epoch 3: val_loss improved from 1.62177 to 1.58456, saving model to final_conv2d+lstm_model_2023-02-1204_52_58.729297/model-00003-0.73060-0.74510-1.58456-0.27000.h5\n","34/34 [==============================] - 591s 17s/step - loss: 0.7306 - categorical_accuracy: 0.7451 - val_loss: 1.5846 - val_categorical_accuracy: 0.2700 - lr: 2.0000e-04\n","Epoch 4/20\n","34/34 [==============================] - ETA: 0s - loss: 0.5751 - categorical_accuracy: 0.8160 \n","Epoch 4: val_loss did not improve from 1.58456\n","34/34 [==============================] - 556s 16s/step - loss: 0.5751 - categorical_accuracy: 0.8160 - val_loss: 1.6962 - val_categorical_accuracy: 0.1900 - lr: 2.0000e-04\n","Epoch 5/20\n","34/34 [==============================] - ETA: 0s - loss: 0.4293 - categorical_accuracy: 0.8537 \n","Epoch 5: val_loss did not improve from 1.58456\n","\n","Epoch 5: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n","34/34 [==============================] - 577s 17s/step - loss: 0.4293 - categorical_accuracy: 0.8537 - val_loss: 1.6821 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n","Epoch 6/20\n","34/34 [==============================] - ETA: 0s - loss: 0.2794 - categorical_accuracy: 0.9231 \n","Epoch 6: val_loss did not improve from 1.58456\n","34/34 [==============================] - 585s 17s/step - loss: 0.2794 - categorical_accuracy: 0.9231 - val_loss: 1.7271 - val_categorical_accuracy: 0.2200 - lr: 1.0000e-04\n","Epoch 7/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1970 - categorical_accuracy: 0.9593 \n","Epoch 7: val_loss did not improve from 1.58456\n","\n","Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n","34/34 [==============================] - 584s 17s/step - loss: 0.1970 - categorical_accuracy: 0.9593 - val_loss: 1.7601 - val_categorical_accuracy: 0.2100 - lr: 1.0000e-04\n","Epoch 8/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1528 - categorical_accuracy: 0.9713 \n","Epoch 8: val_loss did not improve from 1.58456\n","34/34 [==============================] - 549s 16s/step - loss: 0.1528 - categorical_accuracy: 0.9713 - val_loss: 1.9546 - val_categorical_accuracy: 0.2500 - lr: 5.0000e-05\n","Epoch 9/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1067 - categorical_accuracy: 0.9894 \n","Epoch 9: val_loss did not improve from 1.58456\n","\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n","34/34 [==============================] - 541s 16s/step - loss: 0.1067 - categorical_accuracy: 0.9894 - val_loss: 2.1162 - val_categorical_accuracy: 0.2900 - lr: 5.0000e-05\n","Epoch 10/20\n","34/34 [==============================] - ETA: 0s - loss: 0.1125 - categorical_accuracy: 0.9834 \n","Epoch 10: val_loss did not improve from 1.58456\n","34/34 [==============================] - 573s 17s/step - loss: 0.1125 - categorical_accuracy: 0.9834 - val_loss: 2.0616 - val_categorical_accuracy: 0.3000 - lr: 2.5000e-05\n","Epoch 11/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0878 - categorical_accuracy: 0.9955 \n","Epoch 11: val_loss did not improve from 1.58456\n","\n","Epoch 11: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n","34/34 [==============================] - 543s 16s/step - loss: 0.0878 - categorical_accuracy: 0.9955 - val_loss: 2.1157 - val_categorical_accuracy: 0.3600 - lr: 2.5000e-05\n","Epoch 12/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0845 - categorical_accuracy: 0.9925 \n","Epoch 12: val_loss did not improve from 1.58456\n","34/34 [==============================] - 513s 15s/step - loss: 0.0845 - categorical_accuracy: 0.9925 - val_loss: 2.0198 - val_categorical_accuracy: 0.3900 - lr: 1.2500e-05\n","Epoch 13/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0756 - categorical_accuracy: 0.9940 \n","Epoch 13: val_loss did not improve from 1.58456\n","\n","Epoch 13: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n","34/34 [==============================] - 546s 16s/step - loss: 0.0756 - categorical_accuracy: 0.9940 - val_loss: 2.1569 - val_categorical_accuracy: 0.4100 - lr: 1.2500e-05\n","Epoch 14/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0822 - categorical_accuracy: 0.9940 \n","Epoch 14: val_loss did not improve from 1.58456\n","34/34 [==============================] - 545s 16s/step - loss: 0.0822 - categorical_accuracy: 0.9940 - val_loss: 1.9439 - val_categorical_accuracy: 0.4800 - lr: 6.2500e-06\n","Epoch 15/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0813 - categorical_accuracy: 0.9925 \n","Epoch 15: val_loss improved from 1.58456 to 1.44212, saving model to final_conv2d+lstm_model_2023-02-1204_52_58.729297/model-00015-0.08128-0.99246-1.44212-0.52000.h5\n","34/34 [==============================] - 541s 16s/step - loss: 0.0813 - categorical_accuracy: 0.9925 - val_loss: 1.4421 - val_categorical_accuracy: 0.5200 - lr: 6.2500e-06\n","Epoch 16/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0773 - categorical_accuracy: 0.9940 \n","Epoch 16: val_loss improved from 1.44212 to 1.29735, saving model to final_conv2d+lstm_model_2023-02-1204_52_58.729297/model-00016-0.07729-0.99397-1.29735-0.57000.h5\n","34/34 [==============================] - 538s 16s/step - loss: 0.0773 - categorical_accuracy: 0.9940 - val_loss: 1.2973 - val_categorical_accuracy: 0.5700 - lr: 6.2500e-06\n","Epoch 17/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0777 - categorical_accuracy: 0.9864 \n","Epoch 17: val_loss improved from 1.29735 to 1.12774, saving model to final_conv2d+lstm_model_2023-02-1204_52_58.729297/model-00017-0.07769-0.98643-1.12774-0.63000.h5\n","34/34 [==============================] - 540s 16s/step - loss: 0.0777 - categorical_accuracy: 0.9864 - val_loss: 1.1277 - val_categorical_accuracy: 0.6300 - lr: 6.2500e-06\n","Epoch 18/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0679 - categorical_accuracy: 0.9955 \n","Epoch 18: val_loss improved from 1.12774 to 1.01322, saving model to final_conv2d+lstm_model_2023-02-1204_52_58.729297/model-00018-0.06794-0.99548-1.01322-0.61000.h5\n","34/34 [==============================] - 549s 16s/step - loss: 0.0679 - categorical_accuracy: 0.9955 - val_loss: 1.0132 - val_categorical_accuracy: 0.6100 - lr: 6.2500e-06\n","Epoch 19/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0704 - categorical_accuracy: 0.9940 \n","Epoch 19: val_loss improved from 1.01322 to 0.89619, saving model to final_conv2d+lstm_model_2023-02-1204_52_58.729297/model-00019-0.07037-0.99397-0.89619-0.72000.h5\n","34/34 [==============================] - 551s 16s/step - loss: 0.0704 - categorical_accuracy: 0.9940 - val_loss: 0.8962 - val_categorical_accuracy: 0.7200 - lr: 6.2500e-06\n","Epoch 20/20\n","34/34 [==============================] - ETA: 0s - loss: 0.0748 - categorical_accuracy: 0.9940 \n","Epoch 20: val_loss did not improve from 0.89619\n","34/34 [==============================] - 551s 16s/step - loss: 0.0748 - categorical_accuracy: 0.9940 - val_loss: 0.9038 - val_categorical_accuracy: 0.7200 - lr: 6.2500e-06\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa38af90220\u003e"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model_2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n","                    callbacks=callbacks_list, validation_data=val_generator, \n","                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"]},{"cell_type":"markdown","metadata":{"id":"jnbeUKyg7hWu"},"source":["## 4 / 5 and 6 Model Class added with newer approach\n"]},{"cell_type":"markdown","metadata":{"id":"G3LrcWL2-QhP"},"source":["### Generator Class\n","This is one of the most important part of the code. The overall structure of the generator is broken down into modules. In the generator, we are going to preprocess the images as we have images of 2 different dimensions as well as create a batch of video frames."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676227986448,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"JbwZiuoo-EXs"},"outputs":[],"source":["class DataGenerator:\n","    def __init__(self, width=120, height=120, frames=30, channel=3, \n","                 crop = True, normalize = False, affine = False, flip = False, edge = False  ):\n","        self.width = width   # X dimension of the image\n","        self.height = height # Y dimesnion of the image\n","        self.frames = frames # length/depth of the video frames\n","        self.channel = channel # number of channels in images 3 for color(RGB) and 1 for Gray  \n","        self.affine = affine # augment data with affine transform of the image\n","        self.flip = flip\n","        self.normalize =  normalize\n","        self.edge = edge # edge detection\n","        self.crop = crop\n","\n","    # Helper function to generate a random affine transform on the image\n","    def __get_random_affine(self): # private method\n","        dx, dy = np.random.randint(-1.7, 1.8, 2)\n","        M = np.float32([[1, 0, dx], [0, 1, dy]])\n","        return M\n","\n","    # Helper function to initialize all the batch image data and labels\n","    def __init_batch_data(self, batch_size): # private method\n","        batch_data = np.zeros((batch_size, self.frames, self.width, self.height, self.channel)) \n","        batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n","        return batch_data, batch_labels\n","\n","    def __load_batch_images(self, source_path, folder_list, batch_num, batch_size, t): # private method\n","    \n","        batch_data,batch_labels = self.__init_batch_data(batch_size)\n","        # We will also build a agumented batch data\n","        if self.affine:\n","            batch_data_aug,batch_labels_aug = self.__init_batch_data(batch_size)\n","        if self.flip:\n","            batch_data_flip,batch_labels_flip = self.__init_batch_data(batch_size)\n","\n","        #create a list of image numbers you want to use for a particular video\n","        img_idx = [x for x in range(0, self.frames)] \n","\n","        for folder in range(batch_size): # iterate over the batch_size\n","            # read all the images in the folder\n","            imgs = sorted(os.listdir(source_path+'/'+ t[folder + (batch_num*batch_size)].split(';')[0])) \n","            # Generate a random affine to be used in image transformation for buidling agumented data set\n","            M = self.__get_random_affine()\n","            \n","            #  Iterate over the frames/images of a folder to read them in\n","            for idx, item in enumerate(img_idx): \n","                image = cv2.imread(source_path+'/'+ t[folder + (batch_num*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n","                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","                \n","                #crop the images and resize them. Note that the images are of 2 different shape \n","                #and the conv3D will throw error if the inputs in a batch have different shapes  \n","                if self.crop:\n","                    image = self.__crop(image)\n","                # If normalize is set normalize the image else use the raw image.\n","                if self.normalize:\n","                    resized = self.__normalize(self.__resize(image))\n","                else:\n","                    resized = self.__resize(image)\n","                # If the input is edge detected image then use the sobelx, sobely and laplacian as 3 channel of the edge detected image\n","                if self.edge:\n","                    resized = self.__edge(resized)\n","                \n","                batch_data[folder,idx] = resized\n","                if self.affine:\n","                    batch_data_aug[folder,idx] = self.__affine(resized, M)   \n","                if self.flip:\n","                    batch_data_flip[folder,idx] = self.__flip(resized)   \n","\n","            batch_labels[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n","            \n","            if self.affine:\n","                batch_labels_aug[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n","            \n","            if self.flip:\n","                if int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==0:\n","                    batch_labels_flip[folder, 1] = 1\n","                elif int(t[folder + (batch_num*batch_size)].strip().split(';')[2])==1:\n","                    batch_labels_flip[folder, 0] = 1\n","                else:\n","                    batch_labels_flip[folder, int(t[folder + (batch_num*batch_size)].strip().split(';')[2])] = 1\n","        \n","        if self.affine:\n","            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n","            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n","        if self.flip:\n","            batch_data = np.append(batch_data, batch_data_flip, axis = 0) \n","            batch_labels = np.append(batch_labels, batch_labels_flip, axis = 0) \n","\n","        return batch_data, batch_labels\n","    \n","    def generator(self, source_path, folder_list, batch_size): # public method\n","        print( 'Source path = ', source_path, '; batch size =', batch_size)\n","        while True:\n","            t = np.random.permutation(folder_list)\n","            num_batches = len(folder_list)//batch_size # calculate the number of batches\n","            for batch in range(num_batches): # we iterate over the number of batches\n","                # you yield the batch_data and the batch_labels, remember what does yield do\n","                yield self.__load_batch_images(source_path, folder_list, batch, batch_size, t) \n","            \n","            # Code for the remaining data points which are left after full batches\n","            if (len(folder_list) != batch_size*num_batches):\n","                batch_size = len(folder_list) - (batch_size*num_batches)\n","                yield self.__load_batch_images(source_path, folder_list, num_batches, batch_size, t)\n","\n","    # Helper function to perfom affice transform on the image\n","    def __affine(self, image, M):\n","        return cv2.warpAffine(image, M, (image.shape[0], image.shape[1]))\n","\n","    # Helper function to flip the image\n","    def __flip(self, image):\n","        return np.flip(image,1)\n","    \n","    # Helper function to normalise the data\n","    def __normalize(self, image):\n","        return image/127.5-1\n","    \n","    # Helper function to resize the image\n","    def __resize(self, image):\n","        return cv2.resize(image, (self.width,self.height), interpolation = cv2.INTER_AREA)\n","    \n","    # Helper function to crop the image\n","    def __crop(self, image):\n","        if image.shape[0] != image.shape[1]:\n","            return image[0:120, 20:140]\n","        else:\n","            return image\n","\n","    # Helper function for edge detection\n","    def __edge(self, image):\n","        edge = np.zeros((image.shape[0], image.shape[1], image.shape[2]))\n","        edge[:,:,0] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,0],(3,3),0),cv2.CV_64F)\n","        edge[:,:,1] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,1],(3,3),0),cv2.CV_64F)\n","        edge[:,:,2] = cv2.Laplacian(cv2.GaussianBlur(image[:,:,2],(3,3),0),cv2.CV_64F)\n","        return edge"]},{"cell_type":"markdown","metadata":{"id":"MlGCUe5M-XBQ"},"source":["### Model Class"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":280,"status":"ok","timestamp":1676228044838,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"oPOlzn1a-UKo"},"outputs":[],"source":["class ModelGenerator(object):\n","        \n","    @classmethod\n","    def c3d3(cls, input_shape, nb_classes):\n","        model = Sequential()\n","        model.add(Conv3D(16, kernel_size=(3, 3, 3), input_shape=input_shape, padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(Conv3D(16, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(Conv3D(32, padding=\"same\", kernel_size=(3, 3, 3)))\n","        model.add(Activation('relu'))\n","        model.add(MaxPooling3D(pool_size=(3, 3, 3), padding=\"same\"))\n","        model.add(Dropout(0.25))\n","\n","        model.add(Flatten())\n","        model.add(Dense(512, activation='relu'))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model\n"," \n","    @classmethod\n","    def c3d4(cls, input_shape, nb_classes):\n","        # Define model\n","        model = Sequential()\n","\n","        model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        #Flatten Layers\n","        model.add(Flatten())\n","\n","        model.add(Dense(256, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        #softmax layer\n","        model.add(Dense(5, activation='softmax'))\n","        return model\n","    \n","    @classmethod\n","    def c3d5(cls, input_shape, nb_classes):\n","        # Define model\n","        model = Sequential()\n","\n","        model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=input_shape, padding='same'))\n","        #model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n","        #model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n","        #model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n","        model.add(Activation('relu'))\n","        model.add(Dropout(0.25))\n","\n","        model.add(MaxPooling3D(pool_size=(2,2,2)))\n","\n","        #Flatten Layers\n","        model.add(Flatten())\n","\n","        model.add(Dense(256, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dropout(0.5))\n","\n","        #softmax layer\n","        model.add(Dense(5, activation='softmax'))\n","        return model   \n","    \n","    @classmethod\n","    def lstm(cls, input_shape, nb_classes):\n","        \"\"\"Build a simple LSTM network. We pass the extracted features from\n","        our CNN to this model predomenently.\"\"\"\n","        # Model.\n","        model = Sequential()\n","        model.add(LSTM(2048, return_sequences=False,\n","                       input_shape=input_shape,\n","                       dropout=0.5))\n","        model.add(Dense(512, activation='relu'))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model\n","\n","    @classmethod\n","    def lrcn(cls, input_shape, nb_classes):\n","        \"\"\"Build a CNN into RNN.\n","        Starting version from:\n","            https://github.com/udacity/self-driving-car/blob/master/\n","                steering-models/community-models/chauffeur/models.py\n","        Heavily influenced by VGG-16:\n","            https://arxiv.org/abs/1409.1556\n","        Also known as an LRCN:\n","            https://arxiv.org/pdf/1411.4389.pdf\n","        \"\"\"\n","        model = Sequential()\n","\n","        model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2),\n","            activation='relu', padding='same'), input_shape=input_shape))\n","        model.add(TimeDistributed(Conv2D(32, (3,3),\n","            kernel_initializer=\"he_normal\", activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Conv2D(64, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(64, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Conv2D(128, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(128, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Conv2D(256, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(256, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","        \n","        model.add(TimeDistributed(Conv2D(512, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(Conv2D(512, (3,3),\n","            padding='same', activation='relu')))\n","        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n","\n","        model.add(TimeDistributed(Flatten()))\n","\n","        model.add(Dropout(0.5))\n","        model.add(LSTM(256, return_sequences=False, dropout=0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model\n","\n","    @classmethod\n","    def mlp(cls, input_shape, nb_classes):\n","        \"\"\"Build a simple MLP. It uses extracted features as the input\n","        because of the otherwise too-high dimensionality.\"\"\"\n","        # Model.\n","        model = Sequential()\n","        model.add(Flatten(input_shape=input_shape))\n","        model.add(Dense(512))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(512))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(nb_classes, activation='softmax'))\n","\n","        return model"]},{"cell_type":"markdown","metadata":{"id":"s7iQ3jYW-mkW"},"source":["### Trainer Function"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676228209852,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"LCZHcNXq-iX4"},"outputs":[],"source":["def train(batch_size, num_epochs, model, train_generator, val_generator, optimiser=None):\n","\n","    curr_dt_time = datetime.datetime.now()\n","\n","    num_train_sequences = len(train_doc)\n","    print('# training sequences =', num_train_sequences)\n","    num_val_sequences = len(val_doc)\n","    print('# validation sequences =', num_val_sequences)\n","    print('# batch size =', batch_size)    \n","    print('# epochs =', num_epochs)\n","\n","    #optimizer = Adam(lr=rate) \n","    #write your optimizer\n","    if optimiser == None:\n","        optimiser = Adam() \n","    model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","    print (model.summary())\n","    \n","    model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n","    \n","    if not os.path.exists(model_name):\n","        os.mkdir(model_name)\n","            \n","    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","    checkpoint = ModelCheckpoint(filepath, \n","                                 monitor='val_loss', \n","                                 verbose=1, \n","                                 save_best_only=False, \n","                                 save_weights_only=False, \n","                                 mode='auto', \n","                                 period=1)\n","    LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n","    callbacks_list = [checkpoint, LR]\n","\n","    if (num_train_sequences%batch_size) == 0:\n","        steps_per_epoch = int(num_train_sequences/batch_size)\n","    else:\n","        steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","    if (num_val_sequences%batch_size) == 0:\n","        validation_steps = int(num_val_sequences/batch_size)\n","    else:\n","        validation_steps = (num_val_sequences//batch_size) + 1\n","\n","    model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n","                callbacks=callbacks_list, validation_data=val_generator, \n","                validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n","    \n","    K.clear_session()"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":1009,"status":"ok","timestamp":1676228081811,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"a62bF527-ppv"},"outputs":[],"source":["from numba import cuda\n","def clear_cuda():\n","    #cuda.select_device(0)\n","    #cuda.close()\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"5Ywn00tG_MSF"},"source":["### Model 3"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":515,"status":"ok","timestamp":1676228258419,"user":{"displayName":"Richard Sam","userId":"02214909333000238853"},"user_tz":300},"id":"L7iKMDSD_WHL"},"outputs":[],"source":["import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Vy0iiiXl-rKv"},"outputs":[{"name":"stdout","output_type":"stream","text":["# training sequences = 663\n","# validation sequences = 100\n","# batch size = 20\n","# epochs = 20\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv3d_34 (Conv3D)          (None, 30, 120, 120, 16)  1312      \n","                                                                 \n"," activation_34 (Activation)  (None, 30, 120, 120, 16)  0         \n","                                                                 \n"," conv3d_35 (Conv3D)          (None, 30, 120, 120, 16)  6928      \n","                                                                 \n"," activation_35 (Activation)  (None, 30, 120, 120, 16)  0         \n","                                                                 \n"," max_pooling3d_17 (MaxPoolin  (None, 10, 40, 40, 16)   0         \n"," g3D)                                                            \n","                                                                 \n"," dropout_16 (Dropout)        (None, 10, 40, 40, 16)    0         \n","                                                                 \n"," conv3d_36 (Conv3D)          (None, 10, 40, 40, 32)    13856     \n","                                                                 \n"," activation_36 (Activation)  (None, 10, 40, 40, 32)    0         \n","                                                                 \n"," conv3d_37 (Conv3D)          (None, 10, 40, 40, 32)    27680     \n","                                                                 \n"," activation_37 (Activation)  (None, 10, 40, 40, 32)    0         \n","                                                                 \n"," max_pooling3d_18 (MaxPoolin  (None, 4, 14, 14, 32)    0         \n"," g3D)                                                            \n","                                                                 \n"," dropout_17 (Dropout)        (None, 4, 14, 14, 32)     0         \n","                                                                 \n"," conv3d_38 (Conv3D)          (None, 4, 14, 14, 32)     27680     \n","                                                                 \n"," activation_38 (Activation)  (None, 4, 14, 14, 32)     0         \n","                                                                 \n"," conv3d_39 (Conv3D)          (None, 4, 14, 14, 32)     27680     \n","                                                                 \n"," activation_39 (Activation)  (None, 4, 14, 14, 32)     0         \n","                                                                 \n"," max_pooling3d_19 (MaxPoolin  (None, 2, 5, 5, 32)      0         \n"," g3D)                                                            \n","                                                                 \n"," dropout_18 (Dropout)        (None, 2, 5, 5, 32)       0         \n","                                                                 \n"," flatten_5 (Flatten)         (None, 1600)              0         \n","                                                                 \n"," dense_12 (Dense)            (None, 512)               819712    \n","                                                                 \n"," batch_normalization_23 (Bat  (None, 512)              2048      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_19 (Dropout)        (None, 512)               0         \n","                                                                 \n"," dense_13 (Dense)            (None, 5)                 2565      \n","                                                                 \n","=================================================================\n","Total params: 929,461\n","Trainable params: 928,437\n","Non-trainable params: 1,024\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"]},{"name":"stdout","output_type":"stream","text":["None\n","Source path =  /content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/train ; batch size = 20\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-25-555dec366f02\u003e:46: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","34/34 [==============================] - ETA: 0s - loss: 1.9667 - categorical_accuracy: 0.3348  Source path =  /content/drive/MyDrive/Upgrad/ML2/NNET Chess/Neural_Net_Gesture_Recognition/val ; batch size = 20\n","\n","Epoch 1: saving model to model_init_2023-02-1218_57_40.780465/model-00001-1.96674-0.33484-21.13355-0.25000.h5\n","34/34 [==============================] - 3816s 116s/step - loss: 1.9667 - categorical_accuracy: 0.3348 - val_loss: 21.1336 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n","Epoch 2/20\n","34/34 [==============================] - ETA: 0s - loss: 2.0861 - categorical_accuracy: 0.3137\n","Epoch 2: saving model to model_init_2023-02-1218_57_40.780465/model-00002-2.08611-0.31373-12.76117-0.17000.h5\n","34/34 [==============================] - 26s 800ms/step - loss: 2.0861 - categorical_accuracy: 0.3137 - val_loss: 12.7612 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n","Epoch 3/20\n","34/34 [==============================] - ETA: 0s - loss: 2.0875 - categorical_accuracy: 0.2843\n","Epoch 3: saving model to model_init_2023-02-1218_57_40.780465/model-00003-2.08751-0.28431-12.55077-0.27000.h5\n","34/34 [==============================] - 31s 941ms/step - loss: 2.0875 - categorical_accuracy: 0.2843 - val_loss: 12.5508 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n","Epoch 4/20\n","34/34 [==============================] - ETA: 0s - loss: 2.0247 - categorical_accuracy: 0.2745\n","Epoch 4: saving model to model_init_2023-02-1218_57_40.780465/model-00004-2.02475-0.27451-3.75849-0.27000.h5\n","34/34 [==============================] - 29s 889ms/step - loss: 2.0247 - categorical_accuracy: 0.2745 - val_loss: 3.7585 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n","Epoch 5/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6796 - categorical_accuracy: 0.4020\n","Epoch 5: saving model to model_init_2023-02-1218_57_40.780465/model-00005-1.67961-0.40196-13.77384-0.22000.h5\n","34/34 [==============================] - 27s 804ms/step - loss: 1.6796 - categorical_accuracy: 0.4020 - val_loss: 13.7738 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n","Epoch 6/20\n","34/34 [==============================] - ETA: 0s - loss: 1.8909 - categorical_accuracy: 0.2549\n","Epoch 6: saving model to model_init_2023-02-1218_57_40.780465/model-00006-1.89087-0.25490-6.29520-0.18000.h5\n","\n","Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","34/34 [==============================] - 27s 804ms/step - loss: 1.8909 - categorical_accuracy: 0.2549 - val_loss: 6.2952 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n","Epoch 7/20\n","34/34 [==============================] - ETA: 0s - loss: 1.7272 - categorical_accuracy: 0.3627\n","Epoch 7: saving model to model_init_2023-02-1218_57_40.780465/model-00007-1.72717-0.36275-2.65571-0.25000.h5\n","34/34 [==============================] - 28s 846ms/step - loss: 1.7272 - categorical_accuracy: 0.3627 - val_loss: 2.6557 - val_categorical_accuracy: 0.2500 - lr: 5.0000e-04\n","Epoch 8/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6518 - categorical_accuracy: 0.2941\n","Epoch 8: saving model to model_init_2023-02-1218_57_40.780465/model-00008-1.65181-0.29412-1.54589-0.35000.h5\n","34/34 [==============================] - 26s 793ms/step - loss: 1.6518 - categorical_accuracy: 0.2941 - val_loss: 1.5459 - val_categorical_accuracy: 0.3500 - lr: 5.0000e-04\n","Epoch 9/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5002 - categorical_accuracy: 0.3725\n","Epoch 9: saving model to model_init_2023-02-1218_57_40.780465/model-00009-1.50023-0.37255-1.53317-0.41000.h5\n","34/34 [==============================] - 27s 819ms/step - loss: 1.5002 - categorical_accuracy: 0.3725 - val_loss: 1.5332 - val_categorical_accuracy: 0.4100 - lr: 5.0000e-04\n","Epoch 10/20\n","34/34 [==============================] - ETA: 0s - loss: 1.4414 - categorical_accuracy: 0.4020\n","Epoch 10: saving model to model_init_2023-02-1218_57_40.780465/model-00010-1.44137-0.40196-1.28573-0.43000.h5\n","34/34 [==============================] - 26s 794ms/step - loss: 1.4414 - categorical_accuracy: 0.4020 - val_loss: 1.2857 - val_categorical_accuracy: 0.4300 - lr: 5.0000e-04\n","Epoch 11/20\n","34/34 [==============================] - ETA: 0s - loss: 1.3978 - categorical_accuracy: 0.3431\n","Epoch 11: saving model to model_init_2023-02-1218_57_40.780465/model-00011-1.39778-0.34314-2.06008-0.30000.h5\n","34/34 [==============================] - 26s 783ms/step - loss: 1.3978 - categorical_accuracy: 0.3431 - val_loss: 2.0601 - val_categorical_accuracy: 0.3000 - lr: 5.0000e-04\n","Epoch 12/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5825 - categorical_accuracy: 0.3529\n","Epoch 12: saving model to model_init_2023-02-1218_57_40.780465/model-00012-1.58247-0.35294-2.18485-0.38000.h5\n","\n","Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","34/34 [==============================] - 26s 777ms/step - loss: 1.5825 - categorical_accuracy: 0.3529 - val_loss: 2.1848 - val_categorical_accuracy: 0.3800 - lr: 5.0000e-04\n","Epoch 13/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5474 - categorical_accuracy: 0.3922\n","Epoch 13: saving model to model_init_2023-02-1218_57_40.780465/model-00013-1.54740-0.39216-2.96161-0.27000.h5\n","34/34 [==============================] - 27s 815ms/step - loss: 1.5474 - categorical_accuracy: 0.3922 - val_loss: 2.9616 - val_categorical_accuracy: 0.2700 - lr: 2.5000e-04\n","Epoch 14/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5486 - categorical_accuracy: 0.3725\n","Epoch 14: saving model to model_init_2023-02-1218_57_40.780465/model-00014-1.54862-0.37255-1.77462-0.33000.h5\n","\n","Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","34/34 [==============================] - 26s 774ms/step - loss: 1.5486 - categorical_accuracy: 0.3725 - val_loss: 1.7746 - val_categorical_accuracy: 0.3300 - lr: 2.5000e-04\n","Epoch 15/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5146 - categorical_accuracy: 0.3922\n","Epoch 15: saving model to model_init_2023-02-1218_57_40.780465/model-00015-1.51463-0.39216-1.70378-0.37000.h5\n","34/34 [==============================] - 27s 812ms/step - loss: 1.5146 - categorical_accuracy: 0.3922 - val_loss: 1.7038 - val_categorical_accuracy: 0.3700 - lr: 1.2500e-04\n","Epoch 16/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6008 - categorical_accuracy: 0.3627\n","Epoch 16: saving model to model_init_2023-02-1218_57_40.780465/model-00016-1.60082-0.36275-1.21514-0.52000.h5\n","34/34 [==============================] - 26s 786ms/step - loss: 1.6008 - categorical_accuracy: 0.3627 - val_loss: 1.2151 - val_categorical_accuracy: 0.5200 - lr: 1.2500e-04\n","Epoch 17/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5259 - categorical_accuracy: 0.4216\n","Epoch 17: saving model to model_init_2023-02-1218_57_40.780465/model-00017-1.52587-0.42157-1.67958-0.40000.h5\n","34/34 [==============================] - 26s 795ms/step - loss: 1.5259 - categorical_accuracy: 0.4216 - val_loss: 1.6796 - val_categorical_accuracy: 0.4000 - lr: 1.2500e-04\n","Epoch 18/20\n","34/34 [==============================] - ETA: 0s - loss: 1.4972 - categorical_accuracy: 0.3529\n","Epoch 18: saving model to model_init_2023-02-1218_57_40.780465/model-00018-1.49720-0.35294-1.47213-0.37000.h5\n","\n","Epoch 18: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","34/34 [==============================] - 27s 816ms/step - loss: 1.4972 - categorical_accuracy: 0.3529 - val_loss: 1.4721 - val_categorical_accuracy: 0.3700 - lr: 1.2500e-04\n","Epoch 19/20\n","34/34 [==============================] - ETA: 0s - loss: 1.6212 - categorical_accuracy: 0.3627\n","Epoch 19: saving model to model_init_2023-02-1218_57_40.780465/model-00019-1.62123-0.36275-1.15983-0.52000.h5\n","34/34 [==============================] - 25s 768ms/step - loss: 1.6212 - categorical_accuracy: 0.3627 - val_loss: 1.1598 - val_categorical_accuracy: 0.5200 - lr: 6.2500e-05\n","Epoch 20/20\n","34/34 [==============================] - ETA: 0s - loss: 1.5972 - categorical_accuracy: 0.3235\n","Epoch 20: saving model to model_init_2023-02-1218_57_40.780465/model-00020-1.59721-0.32353-1.26871-0.53000.h5\n","34/34 [==============================] - 26s 781ms/step - loss: 1.5972 - categorical_accuracy: 0.3235 - val_loss: 1.2687 - val_categorical_accuracy: 0.5300 - lr: 6.2500e-05\n"]}],"source":["train_gen = DataGenerator()\n","val_gen = DataGenerator()\n","model_gen = ModelGenerator()\n","\n","input_shape = (30,120,120, 3)\n","num_classes = 5\n","\n","model = model_gen.c3d3(input_shape, num_classes)\n","\n","#batch_size = 20\n","#num_epochs = 20\n","\n","train_generator = train_gen.generator(train_path, train_doc, batch_size)\n","val_generator = val_gen.generator(val_path, val_doc, batch_size)\n","train(batch_size, num_epochs, model, train_generator, val_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7A_IzXA-1bg"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","toc_visible":true,"version":""},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}